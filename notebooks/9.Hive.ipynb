{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive\n",
    "![Hive](https://hive.apache.org/images/hive_logo_medium.jpg)\n",
    "\n",
    "- https://hive.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- version 3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
      "export PDSH_RCMD_TYPE=ssh\n",
      "\n",
      "export HADOOP_HOME=/opt/hadoop\n",
      "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
      "export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
      "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
      "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
      "export HADOOP_YARN_HOME=${HADOOP_HOME}\n",
      "\n",
      "export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin     \n",
      "\n",
      "# Flume\n",
      "export FLUME_HOME=/opt/flume\n",
      "export PATH=${PATH}:${FLUME_HOME}/bin\n",
      "\n",
      "# Sqoop\n",
      "export SQOOP_HOME=/opt/sqoop\n",
      "export PATH=${PATH}:${SQOOP_HOME}/bin\n",
      "\n",
      "# Pig\n",
      "export PIG_HOME=/opt/pig\n",
      "export PATH=${PATH}:${PIG_HOME}/bin\n",
      "\n",
      "# Hive\n",
      "export HIVE_HOME=/opt/hive\n",
      "export PATH=${PATH}:${HIVE_HOME}/bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Download package\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://downloads.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz\n",
    "\n",
    "# unpack file and create link\n",
    "tar -zxf apache-hive-3.1.2-bin.tar.gz -C /opt\n",
    "ln -s /opt/apache-hive-3.1.2-bin /opt/hive\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Hive\n",
    "export HIVE_HOME=/opt/hive\n",
    "export PATH=\\${PATH}:\\${HIVE_HOME}/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "# Fix guava and slf4j versions\n",
    "rm /opt/hive/lib/guava-19.0.jar\n",
    "cp /opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar /opt/hive/lib\n",
    "rm /opt/hive/lib/log4j-slf4j-impl-2.10.0.jar\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HOSTNAME': 'hadoop',\n",
       " 'OLDPWD': '/',\n",
       " 'PWD': '/opt',\n",
       " 'HOME': '/home/hadoop',\n",
       " 'SHELL': '/bin/bash',\n",
       " 'SHLVL': '1',\n",
       " 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin:/opt/hadoop/sbin:/opt/flume/bin:/opt/sqoop/bin:/opt/pig/bin:/opt/hive/bin',\n",
       " '_': '/usr/bin/nohup',\n",
       " 'LANGUAGE': 'en.UTF-8',\n",
       " 'LANG': 'en.UTF-8',\n",
       " 'JPY_PARENT_PID': '1566',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
       " 'JAVA_HOME': '/usr/lib/jvm/java-1.8.0-openjdk-amd64',\n",
       " 'PDSH_RCMD_TYPE': 'ssh',\n",
       " 'HADOOP_HOME': '/opt/hadoop',\n",
       " 'HADOOP_COMMON_HOME': '/opt/hadoop',\n",
       " 'HADOOP_CONF_DIR': '/opt/hadoop/etc/hadoop',\n",
       " 'HADOOP_HDFS_HOME': '/opt/hadoop',\n",
       " 'HADOOP_MAPRED_HOME': '/opt/hadoop',\n",
       " 'HADOOP_YARN_HOME': '/opt/hadoop',\n",
       " 'FLUME_HOME': '/opt/flume',\n",
       " 'SQOOP_HOME': '/opt/sqoop',\n",
       " 'PIG_HOME': '/opt/pig',\n",
       " 'HIVE_HOME': '/opt/hive'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv -o /opt/envvars.sh\n",
    "%env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop configuration (for beeline)\n",
    "\n",
    "- core-site.xml\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "...\n",
    "<property>\n",
    "  <name>hadoop.proxyuser.hadoop.groups</name>\n",
    "  <value>*</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>hadoop.proxyuser.hadoop.hosts</name>\n",
    "  <value>*</value>\n",
    "</property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Metastore\n",
    "\n",
    "- using local Derby database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directory in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -mkdir -p /user/hive/warehouse\n",
    "hdfs dfs -chmod g+w /user/hive/warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metastore connection URL:\t jdbc:derby:;databaseName=metastore_db;create=true\n",
      "Metastore Connection Driver :\t org.apache.derby.jdbc.EmbeddedDriver\n",
      "Metastore connection User:\t APP\n",
      "Starting metastore schema initialization to 3.1.0\n",
      "Initialization script hive-schema-3.1.0.derby.sql\n",
      "Initialization script completed\n",
      "schemaTool completed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir $HIVE_HOME/hiveserver2\n",
    "cd $HIVE_HOME/hiveserver2\n",
    "$HIVE_HOME/bin/schematool -dbType derby -initSchema 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start hiveserver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/hive/hiveserver2\n",
    "nohup /opt/hive/bin/hive --service hiveserver2 \\\n",
    "--hiveconf hive.security.authorization.createtable.owner.grants=ALL \\\n",
    "--hiveconf hive.root.logger=INFO,console > hiveserver2.out 2>&1 &\n",
    "echo $! > hiveserver2.pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "- SF Bay Area Bike Share (https://www.kaggle.com/benhamner/sf-bay-area-bike-share)\n",
    "- stations.csv and trips.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stations.csv\n",
      "trips.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-29 16:53:29,360 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-29 16:53:41,312 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-29 16:53:43,882 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir /opt/datasets/bikeshare\n",
    "tar -zxf hivedataset.tgz -C /opt/datasets/bikeshare\n",
    "\n",
    "cd /opt/datasets/bikeshare\n",
    "ls\n",
    "\n",
    "hdfs dfs -mkdir -p bikeshare/stations\n",
    "hdfs dfs -put stations.csv bikeshare/stations\n",
    "hdfs dfs -mkdir -p bikeshare/trips\n",
    "hdfs dfs -put trips.csv bikeshare/trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect with beeline\n",
    "\n",
    "1. Run beeline\n",
    "\n",
    "```bash\n",
    "source /opt/envvars.sh\n",
    "beeline -n hadoop -u jdbc:hive2://localhost:10000\n",
    "```\n",
    "\n",
    "2. Configure jobs executor\n",
    "\n",
    "```sql\n",
    "SET hive.execution.engine=mr;\n",
    "SET mapreduce.framework.name=yarn;\n",
    "```\n",
    "\n",
    "3. Create bikeshare database\n",
    "\n",
    "```sql\n",
    "CREATE DATABASE bikeshare;\n",
    "SHOW DATABASES;\n",
    "USE bikeshare;\n",
    "```\n",
    "\n",
    "4. Create stations table\n",
    "\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE stations (\n",
    "station_id INT,\n",
    "name STRING,\n",
    "lat DOUBLE,\n",
    "long DOUBLE,\n",
    "dockcount INT,\n",
    "landmark STRING,\n",
    "installation STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs:///user/hadoop/bikeshare/stations';\n",
    "```\n",
    "\n",
    "5. Create trips table\n",
    "\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE trips (\n",
    "trip_id INT,\n",
    "duration INT,\n",
    "start_date STRING,\n",
    "start_station STRING,\n",
    "start_terminal INT,\n",
    "end_date STRING,\n",
    "end_station STRING,\n",
    "end_terminal INT,\n",
    "bike_num INT,\n",
    "subscription_type STRING,\n",
    "zip_code STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs:///user/hadoop/bikeshare/trips';\n",
    "```\n",
    "\n",
    "6. Show tables\n",
    "\n",
    "```sql\n",
    "SHOW TABLES;\n",
    "DESCRIBE stations;\n",
    "DESCRIBE trips;\n",
    "DESCRIBE FORMATTED stations;\n",
    "DESCRIBE FORMATTED trips;\n",
    "```\n",
    "\n",
    "7. Run query - number of trips per terminal\n",
    "\n",
    "```sql\n",
    "SELECT start_terminal, start_station, COUNT(1) AS count\n",
    "FROM trips\n",
    "GROUP BY start_terminal, start_station\n",
    "ORDER BY count\n",
    "DESC LIMIT 10;\n",
    "```\n",
    "\n",
    "8. Run query - join between stations and trips\n",
    "\n",
    "```sql\n",
    "SELECT t.trip_id, t.duration, t.start_date, s.name, s.lat, s.long, s.landmark\n",
    "FROM stations s\n",
    "JOIN trips t ON s.station_id = t.start_terminal\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "9. Exit beeline\n",
    "\n",
    "```sql\n",
    "!quit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/hive/hiveserver2\n",
    "\n",
    "# kill hiveserver2\n",
    "kill $(cat hiveserver2.pid)\n",
    "rm hiveserver2.pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
