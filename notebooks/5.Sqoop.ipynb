{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sqoop\n",
    "![Sqoop](https://sqoop.apache.org/images/sqoop-logo.png)\n",
    "\n",
    "- https://sqoop.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- download from https://downloads.apache.org/sqoop/1.4.7\n",
    "- version 1.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
      "export PDSH_RCMD_TYPE=ssh\n",
      "\n",
      "export HADOOP_HOME=/opt/hadoop\n",
      "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
      "export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
      "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
      "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
      "export HADOOP_YARN_HOME=${HADOOP_HOME}\n",
      "\n",
      "export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin     \n",
      "\n",
      "# Flume\n",
      "export FLUME_HOME=/opt/flume\n",
      "export PATH=${PATH}:${FLUME_HOME}/bin\n",
      "\n",
      "# Sqoop\n",
      "export SQOOP_HOME=/opt/sqoop\n",
      "export PATH=${PATH}:${SQOOP_HOME}/bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Download package\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://downloads.apache.org/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz\n",
    "\n",
    "# unpack file and create link\n",
    "tar -zxf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /opt\n",
    "ln -s /opt/sqoop-1.4.7.bin__hadoop-2.6.0 /opt/sqoop\n",
    "\n",
    "# update commons-lang\n",
    "rm /opt/sqoop/lib/commons-lang3-3.4.jar\n",
    "cp /opt/hadoop/share/hadoop/yarn/timelineservice/lib/commons-lang-2.6.jar /opt/sqoop/lib\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Sqoop\n",
    "export SQOOP_HOME=/opt/sqoop\n",
    "export PATH=\\${PATH}:\\${SQOOP_HOME}/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HOSTNAME': 'hadoop',\n",
       " 'OLDPWD': '/',\n",
       " 'PWD': '/opt',\n",
       " 'HOME': '/home/hadoop',\n",
       " 'SHELL': '/bin/bash',\n",
       " 'SHLVL': '1',\n",
       " 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin:/opt/hadoop/sbin:/opt/flume/bin:/opt/sqoop/bin',\n",
       " '_': '/usr/bin/nohup',\n",
       " 'LANGUAGE': 'en.UTF-8',\n",
       " 'LANG': 'en.UTF-8',\n",
       " 'JPY_PARENT_PID': '1566',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
       " 'JAVA_HOME': '/usr/lib/jvm/java-1.8.0-openjdk-amd64',\n",
       " 'PDSH_RCMD_TYPE': 'ssh',\n",
       " 'HADOOP_HOME': '/opt/hadoop',\n",
       " 'HADOOP_COMMON_HOME': '/opt/hadoop',\n",
       " 'HADOOP_CONF_DIR': '/opt/hadoop/etc/hadoop',\n",
       " 'HADOOP_HDFS_HOME': '/opt/hadoop',\n",
       " 'HADOOP_MAPRED_HOME': '/opt/hadoop',\n",
       " 'HADOOP_YARN_HOME': '/opt/hadoop',\n",
       " 'FLUME_HOME': '/opt/flume',\n",
       " 'SQOOP_HOME': '/opt/sqoop'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv -o /opt/envvars.sh\n",
    "%env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mysql-connector\n",
    "\n",
    "- https://dev.mysql.com/downloads/connector/j/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package mysql-connector-java.\n",
      "(Reading database ... 38151 files and directories currently installed.)\n",
      "Preparing to unpack mysql-connector-java_8.0.22-1ubuntu18.04_all.deb ...\n",
      "Unpacking mysql-connector-java (8.0.22-1ubuntu18.04) ...\n",
      "Setting up mysql-connector-java (8.0.22-1ubuntu18.04) ...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Download package\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java_8.0.22-1ubuntu18.04_all.deb\n",
    "    \n",
    "sudo dpkg -i mysql-connector-java_8.0.22-1ubuntu18.04_all.deb\n",
    "\n",
    "cp /usr/share/java/mysql-connector-java-8.0.22.jar /opt/sqoop/lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mysql installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Stopping MySQL database server mysqld\n",
      "   ...done.\n",
      " * Starting MySQL database server mysqld\n",
      "   ...done.\n",
      " * /usr/bin/mysqladmin  Ver 8.42 Distrib 5.7.32, for Linux on x86_64\n",
      "Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.\n",
      "\n",
      "Oracle is a registered trademark of Oracle Corporation and/or its\n",
      "affiliates. Other names may be trademarks of their respective\n",
      "owners.\n",
      "\n",
      "Server version\t\t5.7.32-0ubuntu0.18.04.1\n",
      "Protocol version\t10\n",
      "Connection\t\tLocalhost via UNIX socket\n",
      "UNIX socket\t\t/var/run/mysqld/mysqld.sock\n",
      "Uptime:\t\t\t1 sec\n",
      "\n",
      "Threads: 1  Questions: 8  Slow queries: 0  Opens: 105  Flush tables: 1  Open tables: 98  Queries per second avg: 8.000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "sudo apt install -qq -y mysql-server unzip >> /tmp/install.log 2>&1\n",
    "\n",
    "# Enable external access (from worker nodes)\n",
    "sudo sed -i \"s/^bind-address/#bind-address/g\" /etc/mysql/mysql.conf.d/mysqld.cnf \n",
    "\n",
    "sudo service mysql restart\n",
    "sudo service mysql status\n",
    "\n",
    "# create hadoop user\n",
    "sudo mysql -e \"create user 'hadoop'\"\n",
    "sudo mysql -e \"grant all privileges on *.* to 'hadoop'@'%'\"\n",
    "sudo mysql -e \"flush privileges\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employees database setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  master.zip\n",
      "e5f310ac7786a2a181a7fc124973725d7aa4ce7c\n",
      "   creating: test_db-master/\n",
      "  inflating: test_db-master/Changelog  \n",
      "  inflating: test_db-master/README.md  \n",
      "  inflating: test_db-master/employees.sql  \n",
      "  inflating: test_db-master/employees_partitioned.sql  \n",
      "  inflating: test_db-master/employees_partitioned_5.1.sql  \n",
      "   creating: test_db-master/images/\n",
      "  inflating: test_db-master/images/employees.gif  \n",
      "  inflating: test_db-master/images/employees.jpg  \n",
      "  inflating: test_db-master/images/employees.png  \n",
      "  inflating: test_db-master/load_departments.dump  \n",
      "  inflating: test_db-master/load_dept_emp.dump  \n",
      "  inflating: test_db-master/load_dept_manager.dump  \n",
      "  inflating: test_db-master/load_employees.dump  \n",
      "  inflating: test_db-master/load_salaries1.dump  \n",
      "  inflating: test_db-master/load_salaries2.dump  \n",
      "  inflating: test_db-master/load_salaries3.dump  \n",
      "  inflating: test_db-master/load_titles.dump  \n",
      "  inflating: test_db-master/objects.sql  \n",
      "   creating: test_db-master/sakila/\n",
      "  inflating: test_db-master/sakila/README.md  \n",
      "  inflating: test_db-master/sakila/sakila-mv-data.sql  \n",
      "  inflating: test_db-master/sakila/sakila-mv-schema.sql  \n",
      "  inflating: test_db-master/show_elapsed.sql  \n",
      "  inflating: test_db-master/sql_test.sh  \n",
      "  inflating: test_db-master/test_employees_md5.sql  \n",
      "  inflating: test_db-master/test_employees_sha.sql  \n",
      "  inflating: test_db-master/test_versions.sh  \n",
      "INFO\n",
      "CREATING DATABASE STRUCTURE\n",
      "INFO\n",
      "storage engine: InnoDB\n",
      "INFO\n",
      "LOADING departments\n",
      "INFO\n",
      "LOADING employees\n",
      "INFO\n",
      "LOADING dept_emp\n",
      "INFO\n",
      "LOADING dept_manager\n",
      "INFO\n",
      "LOADING titles\n",
      "INFO\n",
      "LOADING salaries\n",
      "data_load_time_diff\n",
      "00:01:38\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Download EmployeesDB sample database\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://github.com/datacharmer/test_db/archive/master.zip\n",
    "\n",
    "unzip master.zip\n",
    "\n",
    "cd test_db-master\n",
    "\n",
    "mysql -u hadoop < employees.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database\n",
      "information_schema\n",
      "employees\n",
      "mysql\n",
      "performance_schema\n",
      "sys\n",
      "\n",
      "========================================\n",
      "\n",
      "Tables_in_employees\n",
      "current_dept_emp\n",
      "departments\n",
      "dept_emp\n",
      "dept_emp_latest_date\n",
      "dept_manager\n",
      "employees\n",
      "salaries\n",
      "titles\n",
      "\n",
      "========================================\n",
      "\n",
      "Field\tType\tNull\tKey\tDefault\tExtra\n",
      "emp_no\tint(11)\tNO\tPRI\tNULL\t\n",
      "birth_date\tdate\tNO\t\tNULL\t\n",
      "first_name\tvarchar(14)\tNO\t\tNULL\t\n",
      "last_name\tvarchar(16)\tNO\t\tNULL\t\n",
      "gender\tenum('M','F')\tNO\t\tNULL\t\n",
      "hire_date\tdate\tNO\t\tNULL\t\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mysql -u hadoop -e 'show databases'\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" | tr ' ' '='\n",
    "\n",
    "mysql -u hadoop -D employees -e 'show tables'\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" | tr ' ' '='\n",
    "\n",
    "mysql -u hadoop -D employees -e 'describe employees'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sqoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/sqoop/../hbase does not exist! HBase imports will fail.\n",
      "Please set $HBASE_HOME to the root of your HBase installation.\n",
      "Warning: /opt/sqoop/../hcatalog does not exist! HCatalog jobs will fail.\n",
      "Please set $HCAT_HOME to the root of your HCatalog installation.\n",
      "Warning: /opt/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "Warning: /opt/sqoop/../zookeeper does not exist! Accumulo imports will fail.\n",
      "Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.\n",
      "information_schema\n",
      "employees\n",
      "mysql\n",
      "performance_schema\n",
      "sys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hadoop/libexec/hadoop-functions.sh: line 2366: HADOOP_ORG.APACHE.SQOOP.SQOOP_USER: bad substitution\n",
      "/opt/hadoop/libexec/hadoop-functions.sh: line 2461: HADOOP_ORG.APACHE.SQOOP.SQOOP_OPTS: bad substitution\n",
      "2021-01-28 18:58:57,924 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7\n",
      "2021-01-28 18:58:58,506 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "sqoop list-databases --connect jdbc:mysql://hadoop --username hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/sqoop/../hbase does not exist! HBase imports will fail.\n",
      "Please set $HBASE_HOME to the root of your HBase installation.\n",
      "Warning: /opt/sqoop/../hcatalog does not exist! HCatalog jobs will fail.\n",
      "Please set $HCAT_HOME to the root of your HCatalog installation.\n",
      "Warning: /opt/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "Warning: /opt/sqoop/../zookeeper does not exist! Accumulo imports will fail.\n",
      "Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.\n",
      "current_dept_emp\n",
      "departments\n",
      "dept_emp\n",
      "dept_emp_latest_date\n",
      "dept_manager\n",
      "employees\n",
      "salaries\n",
      "titles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hadoop/libexec/hadoop-functions.sh: line 2366: HADOOP_ORG.APACHE.SQOOP.SQOOP_USER: bad substitution\n",
      "/opt/hadoop/libexec/hadoop-functions.sh: line 2461: HADOOP_ORG.APACHE.SQOOP.SQOOP_OPTS: bad substitution\n",
      "2021-01-28 18:59:20,246 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7\n",
      "2021-01-28 18:59:21,481 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "sqoop list-tables --connect jdbc:mysql://hadoop/employees --username hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: /opt/sqoop/../hbase does not exist! HBase imports will fail.\n",
      "Please set $HBASE_HOME to the root of your HBase installation.\n",
      "Warning: /opt/sqoop/../hcatalog does not exist! HCatalog jobs will fail.\n",
      "Please set $HCAT_HOME to the root of your HCatalog installation.\n",
      "Warning: /opt/sqoop/../accumulo does not exist! Accumulo imports will fail.\n",
      "Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n",
      "Warning: /opt/sqoop/../zookeeper does not exist! Accumulo imports will fail.\n",
      "Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hadoop/libexec/hadoop-functions.sh: line 2366: HADOOP_ORG.APACHE.SQOOP.SQOOP_USER: bad substitution\n",
      "/opt/hadoop/libexec/hadoop-functions.sh: line 2461: HADOOP_ORG.APACHE.SQOOP.SQOOP_OPTS: bad substitution\n",
      "2021-01-28 18:59:39,078 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7\n",
      "2021-01-28 18:59:39,698 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
      "2021-01-28 18:59:39,698 INFO tool.CodeGenTool: Beginning code generation\n",
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "2021-01-28 18:59:41,295 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `employees` AS t LIMIT 1\n",
      "2021-01-28 18:59:41,415 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `employees` AS t LIMIT 1\n",
      "2021-01-28 18:59:41,439 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop\n",
      "Note: /tmp/sqoop-hadoop/compile/6388e5ca0cbbe93078dc2bd0c8a533b7/employees.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "2021-01-28 18:59:46,848 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/6388e5ca0cbbe93078dc2bd0c8a533b7/employees.jar\n",
      "2021-01-28 18:59:46,888 WARN manager.MySQLManager: It looks like you are importing from mysql.\n",
      "2021-01-28 18:59:46,888 WARN manager.MySQLManager: This transfer can be faster! Use the --direct\n",
      "2021-01-28 18:59:46,888 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.\n",
      "2021-01-28 18:59:46,889 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)\n",
      "2021-01-28 18:59:46,903 INFO mapreduce.ImportJobBase: Beginning import of employees\n",
      "2021-01-28 18:59:46,905 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2021-01-28 18:59:47,453 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "2021-01-28 18:59:50,296 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2021-01-28 18:59:51,312 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-28 18:59:52,153 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-01-28 18:59:52,631 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1611844877680_0006\n",
      "2021-01-28 18:59:52,910 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:53,259 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:53,426 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:53,561 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:53,677 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:53,759 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:53,856 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:53,950 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:54,099 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:54,208 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:54,300 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:54,376 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:54,707 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:54,972 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:55,196 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:55,321 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:55,439 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:55,567 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:55,812 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:55,894 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:55,967 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:56,053 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:56,144 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:56,211 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:56,277 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:56,328 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:56,432 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:56,506 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:56,659 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:56,770 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:56,850 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:56,950 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:57,008 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:57,094 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:57,411 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:57,834 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:57,963 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:58,105 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "2021-01-28 18:59:58,106 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`emp_no`), MAX(`emp_no`) FROM `employees`\n",
      "2021-01-28 18:59:58,116 INFO db.IntegerSplitter: Split size: 122499; Num splits: 4 from: 10001 to: 499999\n",
      "2021-01-28 18:59:58,237 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:58,535 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:58,637 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "2021-01-28 18:59:59,282 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 18:59:59,968 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1611844877680_0006\n",
      "2021-01-28 18:59:59,969 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-01-28 19:00:01,658 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-01-28 19:00:01,659 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-01-28 19:00:01,984 INFO impl.YarnClientImpl: Submitted application application_1611844877680_0006\n",
      "2021-01-28 19:00:02,251 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1611844877680_0006/\n",
      "2021-01-28 19:00:02,254 INFO mapreduce.Job: Running job: job_1611844877680_0006\n",
      "2021-01-28 19:00:24,273 INFO mapreduce.Job: Job job_1611844877680_0006 running in uber mode : false\n",
      "2021-01-28 19:00:24,283 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-01-28 19:01:14,272 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "2021-01-28 19:01:15,397 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2021-01-28 19:01:33,864 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "2021-01-28 19:01:35,936 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-01-28 19:01:38,191 INFO mapreduce.Job: Job job_1611844877680_0006 completed successfully\n",
      "2021-01-28 19:01:38,496 INFO mapreduce.Job: Counters: 34\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=937616\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=464\n",
      "\t\tHDFS: Number of bytes written=13821993\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=5\n",
      "\t\tOther local map tasks=5\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=464304\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=232152\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=232152\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=59430912\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=300024\n",
      "\t\tMap output records=300024\n",
      "\t\tInput split bytes=464\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=3273\n",
      "\t\tCPU time spent (ms)=71260\n",
      "\t\tPhysical memory (bytes) snapshot=791072768\n",
      "\t\tVirtual memory (bytes) snapshot=7706431488\n",
      "\t\tTotal committed heap usage (bytes)=352845824\n",
      "\t\tPeak Map Physical memory (bytes)=199680000\n",
      "\t\tPeak Map Virtual memory (bytes)=1929367552\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=13821993\n",
      "2021-01-28 19:01:38,515 INFO mapreduce.ImportJobBase: Transferred 13.1817 MB in 108.244 seconds (124.7001 KB/sec)\n",
      "2021-01-28 19:01:38,522 INFO mapreduce.ImportJobBase: Retrieved 300024 records.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "sqoop import --connect jdbc:mysql://hadoop/employees --username hadoop --table employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2021-01-28 19:01 employees/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop      4.3 M 2021-01-28 19:01 employees/part-m-00000\n",
      "-rw-r--r--   2 hadoop hadoop      2.4 M 2021-01-28 19:01 employees/part-m-00001\n",
      "-rw-r--r--   2 hadoop hadoop      2.0 M 2021-01-28 19:01 employees/part-m-00002\n",
      "-rw-r--r--   2 hadoop hadoop      4.4 M 2021-01-28 19:01 employees/part-m-00003\n",
      "10001,1953-09-02,Georgi,Facello,M,1986-06-26\n",
      "10002,1964-06-02,Bezalel,Simmel,F,1985-11-21\n",
      "10003,1959-12-03,Parto,Bamford,M,1986-08-28\n",
      "10004,1954-05-01,Chirstian,Koblick,M,1986-12-01\n",
      "10005,1955-01-21,Kyoichi,Maliniak,M,1989-09-12\n",
      "10006,1953-04-20,Anneke,Preusig,F,1989-06-02\n",
      "10007,1957-05-23,Tzvetan,Zielinski,F,1989-02-10\n",
      "10008,1958-02-19,Saniya,Kalloufi,M,1994-09-15\n",
      "10009,1952-04-19,Sumant,Peac,F,1985-02-18\n",
      "10010,1963-06-01,Duangkaew,Piveteau,F,1989-08-24\n",
      "10011,1953-11-07,Mary,Sluis,F,1990-01-22\n",
      "10012,1960-10-04,Patricio,Bridgland,M,1992-12-18\n",
      "10013,1963-06-07,Eberhardt,Terkki,M,1985-10-20\n",
      "10014,1956-02-12,Berni,Genin,M,1987-03-11\n",
      "10015,1959-08-19,Guoxiang,Nooteboom,M,1987-07-02\n",
      "10016,1961-05-02,Kazuhito,Cappelletti,M,1995-01-27\n",
      "10017,1958-07-06,Cristinel,Bouloucos,F,1993-08-03\n",
      "10018,1954-06-19,Kazuhide,Peha,F,1987-04-03\n",
      "10019,1953-01-23,Lillian,Haddadi,M,1999-04-30\n",
      "10020,1952-12-24,Mayuko,Warwick,M,1991-01-26\n",
      "10021,1960-02-20,Ramzi,Erde,M,1988-02-10\n",
      "10022,1952-07-08,Shahaf,Famili,M,1995-08-22\n",
      "10023,1953-09-29"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 19:02:55,257 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -ls -h employees\n",
    "\n",
    "hdfs dfs -head employees/part-m-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Stopping MySQL database server mysqld\n",
      "   ...done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "rm employees.java\n",
    "\n",
    "# Stopping mysql\n",
    "sudo service mysql stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
