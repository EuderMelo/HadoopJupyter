{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached https://files.pythonhosted.org/packages/32/2e/e4585559237787966aad0f8fd0fc31df1c4c9eb0e62de458c5b6cde954eb/python_dotenv-0.15.0-py2.py3-none-any.whl\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-0.15.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'HOSTNAME': 'hadoop',\n",
       " 'OLDPWD': '/',\n",
       " 'PWD': '/opt',\n",
       " 'HOME': '/home/hadoop',\n",
       " 'SHELL': '/bin/bash',\n",
       " 'SHLVL': '1',\n",
       " 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin:/opt/hadoop/sbin',\n",
       " '_': '/usr/bin/nohup',\n",
       " 'LANGUAGE': 'en.UTF-8',\n",
       " 'LANG': 'en.UTF-8',\n",
       " 'JPY_PARENT_PID': '1566',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
       " 'JAVA_HOME': '/usr/lib/jvm/java-1.8.0-openjdk-amd64',\n",
       " 'PDSH_RCMD_TYPE': 'ssh',\n",
       " 'HADOOP_HOME': '/opt/hadoop',\n",
       " 'HADOOP_COMMON_HOME': '/opt/hadoop',\n",
       " 'HADOOP_CONF_DIR': '/opt/hadoop/etc/hadoop',\n",
       " 'HADOOP_HDFS_HOME': '/opt/hadoop',\n",
       " 'HADOOP_MAPRED_HOME': '/opt/hadoop',\n",
       " 'HADOOP_YARN_HOME': '/opt/hadoop'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source /opt/envvars.sh\n",
    "\n",
    "!pip3 install python-dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv -o /opt/envvars.sh\n",
    "%env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - Web Interface\n",
    "\n",
    "- Master node\n",
    "    - NameNode: http://localhost:9870\n",
    "    - Secondary NameNode: http://localhost:9868\n",
    "- Worker node\n",
    "    - hadoop1\n",
    "        - DataNode: http://localhost:9864\n",
    "    - hadoop2\n",
    "        - DataNode: http://localhost:9865\n",
    "    - hadoop3\n",
    "        - DataNode: http://localhost:9866"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hdfs [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "--buildpaths                       attempt to add class files from build tree\n",
      "--config dir                       Hadoop config directory\n",
      "--daemon (start|status|stop)       operate on a daemon\n",
      "--debug                            turn on shell script debug mode\n",
      "--help                             usage information\n",
      "--hostnames list[,of,host,names]   hosts to use in worker mode\n",
      "--hosts filename                   list of hosts to use in worker mode\n",
      "--loglevel level                   set the log4j level for this command\n",
      "--workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "cacheadmin           configure the HDFS cache\n",
      "crypto               configure HDFS encryption zones\n",
      "debug                run a Debug Admin to execute HDFS debug commands\n",
      "dfsadmin             run a DFS admin client\n",
      "dfsrouteradmin       manage Router-based federation\n",
      "ec                   run a HDFS ErasureCoding CLI\n",
      "fsck                 run a DFS filesystem checking utility\n",
      "haadmin              run a DFS HA admin client\n",
      "jmxget               get JMX exported values from NameNode or DataNode.\n",
      "oev                  apply the offline edits viewer to an edits file\n",
      "oiv                  apply the offline fsimage viewer to an fsimage\n",
      "oiv_legacy           apply the offline fsimage viewer to a legacy fsimage\n",
      "storagepolicies      list/get/set/satisfyStoragePolicy block storage policies\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "classpath            prints the class path needed to get the hadoop jar and\n",
      "                     the required libraries\n",
      "dfs                  run a filesystem command on the file system\n",
      "envvars              display computed Hadoop environment variables\n",
      "fetchdt              fetch a delegation token from the NameNode\n",
      "getconf              get config values from configuration\n",
      "groups               get the groups which users belong to\n",
      "lsSnapshottableDir   list all snapshottable dirs owned by the current user\n",
      "snapshotDiff         diff two snapshots of a directory or diff the current\n",
      "                     directory contents with a snapshot\n",
      "version              print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "balancer             run a cluster balancing utility\n",
      "datanode             run a DFS datanode\n",
      "dfsrouter            run the DFS router\n",
      "diskbalancer         Distributes data evenly among disks on a given node\n",
      "journalnode          run the DFS journalnode\n",
      "mover                run a utility to move block replicas across storage types\n",
      "namenode             run the DFS namenode\n",
      "nfs3                 run an NFS version 3 gateway\n",
      "portmap              run a portmap service\n",
      "secondarynamenode    run the DFS secondary namenode\n",
      "sps                  run external storagepolicysatisfier\n",
      "zkfc                 run the ZK Failover Controller daemon\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystem Basic Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "Download books from Gutenberg project (http://www.gutenberg.org/)\n",
    "\n",
    "- Moby Dick; Or, The Whale by Herman Melville\n",
    "- Pride and Prejudice by Jane Austen\n",
    "- Dracula by Bram Stoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books\n",
      "departments.csv\n",
      "donquixote-output.txt\n",
      "donquixote.txt\n",
      "dracula.txt\n",
      "employees.csv\n",
      "mobydick.txt\n",
      "prideandprejudice.txt\n",
      "shakespeare-output.txt\n",
      "shakespeare.txt\n",
      "stations.csv\n",
      "stop-word-list.csv\n",
      "trips.csv\n",
      "ulysses-output.txt\n",
      "ulysses.txt\n",
      "weblog.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "wget -qc http://www.gutenberg.org/files/2701/2701-0.txt -O mobydick.txt\n",
    "wget -qc http://www.gutenberg.org/files/1342/1342-0.txt -O prideandprejudice.txt\n",
    "wget -qc http://www.gutenberg.org/cache/epub/345/pg345.txt -O dracula.txt\n",
    "\n",
    "ls /opt/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "# create gutenberg folder in HDFS\n",
    "# hdfs dfs -mkdir /user/hadoop/gutenberg\n",
    "\n",
    "# copy books to HDFS\n",
    "# hdfs dfs -put * /user/hadoop/gutenberg\n",
    "# hdfs dfs -copyFromLocal * /user/hadoop/gutenberg\n",
    "\n",
    "# list files in HDFS\n",
    "# hdfs dfs -ls /user/hadoop/gutenberg\n",
    "\n",
    "# show first KB of file\n",
    "# hdfs dfs -head /user/hadoop/gutenberg/mobydick.txt\n",
    "\n",
    "# show last KB of file\n",
    "# hdfs dfs -tail /user/hadoop/gutenberg/prideandprejudice.txt\n",
    "\n",
    "# show whole file - CAREFUL\n",
    "# hdfs dfs -cat /user/hadoop/gutenberg/dracula.txt\n",
    "\n",
    "# append file contents to a file in HDFS\n",
    "# hdfs dfs -appendToFile mobydick.txt prideandprejudice.txt dracula.txt /user/hadoop/allbooks.txt\n",
    "\n",
    "# copy allbooks.txt (in HDFS) to gutenberg directory (in HDFS)\n",
    "# hdfs dfs -cp allbooks.txt /user/hadoop/gutenberg\n",
    "# hdfs dfs -ls -h -R\n",
    "\n",
    "# retrieve allbooks.txt from HDFS\n",
    "# hdfs dfs -get allbooks.txt .\n",
    "# hdfs dfs -copyToLocal /user/hadoop/allbooks.txt .\n",
    "\n",
    "# remove file\n",
    "# hdfs dfs -rm allbooks.txt\n",
    "# hdfs dfs -rm /user/hadoop/allbooks.txt\n",
    "\n",
    "# mv file (also used for renaming)\n",
    "# hdfs dfs -mv gutenberg/allbooks.txt gutenberg/books.txt\n",
    "\n",
    "# print statistics on folder\n",
    "# printf \"name\\ttype\\tsize\\treps\\n\"\n",
    "# hdfs dfs -stat \"%n %F %b %r\" /user/hadoop/gutenberg/*\n",
    "\n",
    "# getmerge\n",
    "# hdfs dfs -getmerge /user/hadoop/gutenberg mergebooks.txt\n",
    "\n",
    "# remove directory and files (-R recursive)\n",
    "# hdfs dfs -rm -R /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilization in a MapReduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 17:47:36,916 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 17:47:37,414 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 17:47:37,582 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/datasets\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/gutenberg\n",
    "hdfs dfs -put mobydick.txt prideandprejudice.txt dracula.txt /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 17:48:14,158 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-28 17:48:14,741 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-01-28 17:48:15,402 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1611844877680_0004\n",
      "2021-01-28 17:48:15,711 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 17:48:16,199 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2021-01-28 17:48:16,304 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 17:48:16,420 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 17:48:16,473 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2021-01-28 17:48:16,797 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 17:48:16,882 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1611844877680_0004\n",
      "2021-01-28 17:48:16,882 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-01-28 17:48:17,467 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-01-28 17:48:17,468 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-01-28 17:48:17,743 INFO impl.YarnClientImpl: Submitted application application_1611844877680_0004\n",
      "2021-01-28 17:48:18,175 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1611844877680_0004/\n",
      "2021-01-28 17:48:18,178 INFO mapreduce.Job: Running job: job_1611844877680_0004\n",
      "2021-01-28 17:48:49,129 INFO mapreduce.Job: Job job_1611844877680_0004 running in uber mode : false\n",
      "2021-01-28 17:48:49,136 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-01-28 17:49:13,170 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2021-01-28 17:49:15,306 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-01-28 17:49:25,559 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-01-28 17:49:26,600 INFO mapreduce.Job: Job job_1611844877680_0004 completed successfully\n",
      "2021-01-28 17:49:26,833 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=983104\n",
      "\t\tFILE: Number of bytes written=2870825\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2959461\n",
      "\t\tHDFS: Number of bytes written=566225\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=127776\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=19982\n",
      "\t\tTotal time spent by all map tasks (ms)=63888\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9991\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=63888\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9991\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=16355328\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2557696\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=52900\n",
      "\t\tMap output records=504959\n",
      "\t\tMap output bytes=4838678\n",
      "\t\tMap output materialized bytes=983116\n",
      "\t\tInput split bytes=362\n",
      "\t\tCombine input records=504959\n",
      "\t\tCombine output records=66277\n",
      "\t\tReduce input groups=50206\n",
      "\t\tReduce shuffle bytes=983116\n",
      "\t\tReduce input records=66277\n",
      "\t\tReduce output records=50206\n",
      "\t\tSpilled Records=132554\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=816\n",
      "\t\tCPU time spent (ms)=18960\n",
      "\t\tPhysical memory (bytes) snapshot=1009057792\n",
      "\t\tVirtual memory (bytes) snapshot=7657218048\n",
      "\t\tTotal committed heap usage (bytes)=668991488\n",
      "\t\tPeak Map Physical memory (bytes)=289972224\n",
      "\t\tPeak Map Virtual memory (bytes)=1914802176\n",
      "\t\tPeak Reduce Physical memory (bytes)=154882048\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1919070208\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2959099\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=566225\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# run wordcount application\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.1.jar wordcount \\\n",
    "/user/hadoop/gutenberg /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2021-01-28 17:49 /user/hadoop/gutenberg-output/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop     566225 2021-01-28 17:49 /user/hadoop/gutenberg-output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# list output folder contents\n",
    "hdfs dfs -ls /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"'Are\t1\n",
      "\"'E's\t1\n",
      "\"'I\t1\n",
      "\"'Ittin'\t1\n",
      "\"'Little\t1\n",
      "\"'Lucy,\t1\n",
      "\"'Maybe\t1\n",
      "\"'Miss\t1\n",
      "\"'My\t2\n",
      "\"'Never\t1\n",
      "\"'No'\t1\n",
      "\"'Ow\t1\n",
      "\"'Silence!\t1\n",
      "\"'That's\t1\n",
      "\"'Tyke\t1\n",
      "\"'Wilhelmina'--I\t1\n",
      "\"'Yes,\t1\n",
      "\"A\t8\n",
      "\"ABRAHAM\t1\n",
      "\"ART.\"\t1\n",
      "\"ARTHUR.\"\t1\n",
      "\"About\t1\n",
      "\"Afraid\t1\n",
      "\"Again\t1\n",
      "\"Agreed!\"\t1\n",
      "\"Ah\t2\n",
      "\"Ah,\t16\n",
      "\"Aha!\t1\n",
      "\"Aha!\"\t2\n",
      "\"Alas!\t1\n",
      "\"All\t7\n",
      "\"Already?\"\t1\n",
      "\"Am\t2\n",
      "\"Amen\"\t1\n",
      "\"An\t1\n",
      "\"An'\t1\n",
      "\"And\t49\n",
      "\"And,\t1\n",
      "\"Answer\t1\n",
      "\"Any\t1\n",
      "\"Arabian\t1\n",
      "\"Are\t5\n",
      "\"Arthur\t1\n",
      "\"Arthur!\t2\n",
      "\"As\t6\n",
      "\"Ask\t1\n",
      "\"At\t3\n",
      "\"Ay,\t1\n",
      "\"Back,\t1\n",
      "\"Be\t1\n",
      "\"Because\t7\n",
      "\"Because,\t1\n",
      "\"Because,\"\t2\n",
      "\"Before\t1\n",
      "\"Believe\t3\n",
      "\"Besides,\t1\n",
      "\"Bloofer\t1\n",
      "\"Blow\t1\n",
      "\"Blue\"\t1\n",
      "\"Bother\t1\n",
      "\"Brave\t2\n",
      "\"Bring\t1\n",
      "\"But\t21\n",
      "\"But,\t6\n",
      "\"But,\"\t4\n",
      "\"By\t3\n",
      "\"Call\t1\n",
      "\"Can\t3\n",
      "\"Can't\t1\n",
      "\"Certainly\t2\n",
      "\"Certainly,\"\t1\n",
      "\"Certainly.\"\t1\n",
      "\"Charcot\t1\n",
      "\"Come\t5\n",
      "\"Come!\"\t3\n",
      "\"Come,\t6\n",
      "\"Come,\"\t4\n",
      "\"Come.\t1\n",
      "\"Count\t4\n",
      "\"DEMETER.\"\t1\n",
      "\"DRACULA.\"\t1\n",
      "\"Dear\t5\n",
      "\"Defects,\"\t1\n",
      "\"Denn\t1\n",
      "\"Destroyed?\"\t1\n",
      "\"Did\t3\n",
      "\"Do\t19\n",
      "\"Doctor,\t1\n",
      "\"Don't\t3\n",
      "\"Dr.\t12\n",
      "\"Draw\t1\n",
      "\"Edward\t1\n",
      "\"Euthanasia\"\t1\n",
      "\"Ever\t1\n",
      "\"Exactly.\t1\n",
      "\"Excuse\t1\n",
      "\"FINIS.\"\t1\n",
      "\"Faithfully\t1\n",
      "\"Fear\t1\n",
      "\"Fifty\t2\n",
      "\"Finis,\"\t1\n",
      "\"For\t5\n",
      "\"Forgive\t4\n",
      "\"Forgiven!\t1\n",
      "\"Frankly\t1\n",
      "\"Friend\t10\n",
      "\"Give\t1\n",
      "\"G"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 17:50:37,869 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# show head\n",
    "hdfs dfs -head /user/hadoop/gutenberg-output/part-r-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"'Are\t1\n",
      "\"'E's\t1\n",
      "\"'I\t1\n",
      "\"'Ittin'\t1\n",
      "\"'Little\t1\n",
      "\"'Lucy,\t1\n",
      "\"'Maybe\t1\n",
      "\"'Miss\t1\n",
      "\"'My\t2\n",
      "\"'Never\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 17:51:01,299 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /tmp\n",
    "\n",
    "# copy HDFS file to local filesystem\n",
    "hdfs dfs -get /user/hadoop/gutenberg-output/part-r-00000 gutenberg-output.txt\n",
    "head /tmp/gutenberg-output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/hadoop/gutenberg-output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# remove folder on HDFS\n",
    "hdfs dfs -rm -R /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 17:51:34,437 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-28 17:51:34,974 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-01-28 17:51:35,654 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1611844877680_0005\n",
      "2021-01-28 17:51:35,945 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 17:51:36,386 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2021-01-28 17:51:36,499 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 17:51:36,623 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 17:51:36,670 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2021-01-28 17:51:37,132 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 17:51:37,250 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1611844877680_0005\n",
      "2021-01-28 17:51:37,250 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-01-28 17:51:37,820 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-01-28 17:51:37,821 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-01-28 17:51:38,092 INFO impl.YarnClientImpl: Submitted application application_1611844877680_0005\n",
      "2021-01-28 17:51:38,368 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1611844877680_0005/\n",
      "2021-01-28 17:51:38,370 INFO mapreduce.Job: Running job: job_1611844877680_0005\n",
      "2021-01-28 17:51:53,926 INFO mapreduce.Job: Job job_1611844877680_0005 running in uber mode : false\n",
      "2021-01-28 17:51:53,928 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-01-28 17:52:23,256 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2021-01-28 17:52:27,279 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-01-28 17:52:45,559 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-01-28 17:52:46,593 INFO mapreduce.Job: Job job_1611844877680_0005 completed successfully\n",
      "2021-01-28 17:52:46,773 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=983110\n",
      "\t\tFILE: Number of bytes written=3097026\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2959461\n",
      "\t\tHDFS: Number of bytes written=566225\n",
      "\t\tHDFS: Number of read operations=19\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=168924\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=74048\n",
      "\t\tTotal time spent by all map tasks (ms)=84462\n",
      "\t\tTotal time spent by all reduce tasks (ms)=37024\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=84462\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=37024\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=21622272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9478144\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=52900\n",
      "\t\tMap output records=504959\n",
      "\t\tMap output bytes=4838678\n",
      "\t\tMap output materialized bytes=983134\n",
      "\t\tInput split bytes=362\n",
      "\t\tCombine input records=504959\n",
      "\t\tCombine output records=66277\n",
      "\t\tReduce input groups=50206\n",
      "\t\tReduce shuffle bytes=983134\n",
      "\t\tReduce input records=66277\n",
      "\t\tReduce output records=50206\n",
      "\t\tSpilled Records=132554\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=1229\n",
      "\t\tCPU time spent (ms)=30400\n",
      "\t\tPhysical memory (bytes) snapshot=1194151936\n",
      "\t\tVirtual memory (bytes) snapshot=9570693120\n",
      "\t\tTotal committed heap usage (bytes)=762314752\n",
      "\t\tPeak Map Physical memory (bytes)=290877440\n",
      "\t\tPeak Map Virtual memory (bytes)=1913327616\n",
      "\t\tPeak Reduce Physical memory (bytes)=170455040\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1918717952\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2959099\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=566225\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# run wordcount application with 2 reducers\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.1.jar wordcount \\\n",
    "-Dmapreduce.job.reduces=2 \\\n",
    "/user/hadoop/gutenberg /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2021-01-28 17:52 /user/hadoop/gutenberg-output/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop     282621 2021-01-28 17:52 /user/hadoop/gutenberg-output/part-r-00000\n",
      "-rw-r--r--   2 hadoop hadoop     283604 2021-01-28 17:52 /user/hadoop/gutenberg-output/part-r-00001\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# list output folder contents\n",
    "hdfs dfs -ls /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"'Are\t1\n",
      "\"'Little\t1\n",
      "\"'Maybe\t1\n",
      "\"'Miss\t1\n",
      "\"'My\t2\n",
      "\"'Never\t1\n",
      "\"'No'\t1\n",
      "\"'Ow\t1\n",
      "\"'Silence!\t1\n",
      "\"'Wilhelmina'--I\t1\n",
      "Deleted /user/hadoop/gutenberg-output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 17:53:34,013 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 17:53:34,306 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /tmp\n",
    "\n",
    "# copy HDFS file to local filesystem\n",
    "hdfs dfs -getmerge /user/hadoop/gutenberg-output gutenberg-output.txt\n",
    "head /tmp/gutenberg-output.txt\n",
    "\n",
    "hdfs dfs -rm -R /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Commands\n",
    "\n",
    "- https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify HDFS cluster status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rack: /default-rack\n",
      "   172.17.0.3:9866 (hadoop1)\n",
      "   172.17.0.4:9866 (hadoop2)\n",
      "   172.17.0.5:9866 (hadoop3)\n",
      "\n",
      "\n",
      "========================================\n",
      "\n",
      "Configured Capacity: 303576084480 (282.73 GB)\n",
      "Present Capacity: 258379335828 (240.63 GB)\n",
      "DFS Remaining: 258049622016 (240.33 GB)\n",
      "DFS Used: 329713812 (314.44 MB)\n",
      "DFS Used%: 0.13%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 164403557 (156.79 MB)\n",
      "Non DFS Used: 9840400027 (9.16 GB)\n",
      "DFS Remaining: 86016540672 (80.11 GB)\n",
      "DFS Used%: 0.16%\n",
      "DFS Remaining%: 85.00%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 17:54:15 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:44:27 GMT 2021\n",
      "Num of Blocks: 29\n",
      "\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 110617469 (105.49 MB)\n",
      "Non DFS Used: 9894186115 (9.21 GB)\n",
      "DFS Remaining: 86016540672 (80.11 GB)\n",
      "DFS Used%: 0.11%\n",
      "DFS Remaining%: 85.00%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 17:54:14 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:40:22 GMT 2021\n",
      "Num of Blocks: 22\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 54692786 (52.16 MB)\n",
      "Non DFS Used: 9950110798 (9.27 GB)\n",
      "DFS Remaining: 86016540672 (80.11 GB)\n",
      "DFS Used%: 0.05%\n",
      "DFS Remaining%: 85.00%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 17:54:14 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:46:45 GMT 2021\n",
      "Num of Blocks: 17\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# print topology\n",
    "hdfs dfsadmin -printTopology\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" |tr \" \" \"=\"\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSCK started by hadoop (auth:SIMPLE) from /172.17.0.2 for path /user/hadoop/gutenberg at Thu Jan 28 17:57:24 GMT 2021\n",
      "/user/hadoop/gutenberg <dir>\n",
      "/user/hadoop/gutenberg/dracula.txt 883160 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-1491992694-172.17.0.2-1611844661220:blk_1073741883_1059 len=883160 Live_repl=2  [DatanodeInfoWithStorage[172.17.0.5:9866,DS-2665ec71-1ca6-48d0-9a9f-73461bcfe975,DISK], DatanodeInfoWithStorage[172.17.0.3:9866,DS-c9690db1-9278-47f4-b1b0-94e81a06dfb8,DISK]]\n",
      "\n",
      "/user/hadoop/gutenberg/mobydick.txt 1276201 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-1491992694-172.17.0.2-1611844661220:blk_1073741881_1057 len=1276201 Live_repl=2  [DatanodeInfoWithStorage[172.17.0.5:9866,DS-2665ec71-1ca6-48d0-9a9f-73461bcfe975,DISK], DatanodeInfoWithStorage[172.17.0.4:9866,DS-fa3ecc8f-ca79-4a02-b2be-62d158883402,DISK]]\n",
      "\n",
      "/user/hadoop/gutenberg/prideandprejudice.txt 799738 bytes, replicated: replication=2, 1 block(s):  OK\n",
      "0. BP-1491992694-172.17.0.2-1611844661220:blk_1073741882_1058 len=799738 Live_repl=2  [DatanodeInfoWithStorage[172.17.0.5:9866,DS-2665ec71-1ca6-48d0-9a9f-73461bcfe975,DISK], DatanodeInfoWithStorage[172.17.0.4:9866,DS-fa3ecc8f-ca79-4a02-b2be-62d158883402,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t3\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t1\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t2959099 B\n",
      " Total files:\t3\n",
      " Total blocks (validated):\t3 (avg. block size 986366 B)\n",
      " Minimally replicated blocks:\t3 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t2.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      "FSCK ended at Thu Jan 28 17:57:24 GMT 2021 in 2 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/user/hadoop/gutenberg' is HEALTHY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://hadoop:9870/fsck?ugi=hadoop&files=1&blocks=1&locations=1&path=%2Fuser%2Fhadoop%2Fgutenberg\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# list folder block location\n",
    "#hdfs fsck /user/hadoop/gutenberg -files -blocks -locations\n",
    "\n",
    "# change replication factor of all files in directory to 3\n",
    "#hdfs dfs -setrep 3 /user/hadoop/gutenberg\n",
    "\n",
    "# list folder block location\n",
    "#hdfs fsck /user/hadoop/gutenberg -files -blocks -locations\n",
    "\n",
    "# change replication factor back to 2\n",
    "#hdfs dfs -setrep 2 /user/hadoop/gutenberg\n",
    "\n",
    "# list folder block location\n",
    "hdfs fsck /user/hadoop/gutenberg -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomission nodes\n",
    "\n",
    "- dfs.hosts.exclude in hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh nodes successful\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Decomissioning hadoop1\n",
    "cat > /opt/hadoop/etc/hadoop/dfs.exclude << EOF\n",
    "hadoop1\n",
    "EOF\n",
    "\n",
    "hdfs dfsadmin -refreshNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 202384056320 (188.48 GB)\n",
      "Present Capacity: 172037928158 (160.22 GB)\n",
      "DFS Remaining: 171708358656 (159.92 GB)\n",
      "DFS Used: 329569502 (314.30 MB)\n",
      "DFS Used%: 0.19%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Decommissioned\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 162344960 (154.82 MB)\n",
      "Non DFS Used: 10004819968 (9.32 GB)\n",
      "DFS Remaining: 85854179328 (79.96 GB)\n",
      "DFS Used%: 0.16%\n",
      "DFS Remaining%: 84.84%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 18:00:12 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:44:27 GMT 2021\n",
      "Num of Blocks: 27\n",
      "\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 164804514 (157.17 MB)\n",
      "Non DFS Used: 10002360414 (9.32 GB)\n",
      "DFS Remaining: 85854179328 (79.96 GB)\n",
      "DFS Used%: 0.16%\n",
      "DFS Remaining%: 84.84%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 18:00:11 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:40:22 GMT 2021\n",
      "Num of Blocks: 34\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 164764988 (157.13 MB)\n",
      "Non DFS Used: 10002399940 (9.32 GB)\n",
      "DFS Remaining: 85854179328 (79.96 GB)\n",
      "DFS Used%: 0.16%\n",
      "DFS Remaining%: 84.84%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 18:00:11 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:46:45 GMT 2021\n",
      "Num of Blocks: 34\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# report HDFS status\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh nodes successful\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Recomission all nodes\n",
    "cat > /opt/hadoop/etc/hadoop/dfs.exclude << EOF\n",
    "EOF\n",
    "\n",
    "hdfs dfsadmin -refreshNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 303576084480 (282.73 GB)\n",
      "Present Capacity: 258379124364 (240.63 GB)\n",
      "DFS Remaining: 257995812864 (240.28 GB)\n",
      "DFS Used: 383311500 (365.55 MB)\n",
      "DFS Used%: 0.15%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 89298015 (85.16 MB)\n",
      "Non DFS Used: 9915575201 (9.23 GB)\n",
      "DFS Remaining: 86016471040 (80.11 GB)\n",
      "DFS Used%: 0.09%\n",
      "DFS Remaining%: 85.00%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 18:00:42 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:44:27 GMT 2021\n",
      "Num of Blocks: 15\n",
      "\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 164804514 (157.17 MB)\n",
      "Non DFS Used: 9893668958 (9.21 GB)\n",
      "DFS Remaining: 85962870784 (80.06 GB)\n",
      "DFS Used%: 0.16%\n",
      "DFS Remaining%: 84.95%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 18:00:41 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:40:22 GMT 2021\n",
      "Num of Blocks: 29\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 129208971 (123.22 MB)\n",
      "Non DFS Used: 9875664245 (9.20 GB)\n",
      "DFS Remaining: 86016471040 (80.11 GB)\n",
      "DFS Used%: 0.13%\n",
      "DFS Remaining%: 85.00%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 18:00:41 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:46:45 GMT 2021\n",
      "Num of Blocks: 26\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# report HDFS status\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling datanode failures\n",
    "\n",
    "- timeouts defined in hdfs-site.xml \n",
    "    - dfs.namenode.heartbeat.recheck-interval = 10000 (10 seconds)\n",
    "    - dfs.heartbeat.interval = 3 seconds\n",
    "- timeout = 2 x recheck-interval + 10 x heartbeat.interval\n",
    "    - timeout = 50 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "3s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# get dfs.namenode.heartbeat.recheck-interval\n",
    "hdfs getconf -confKey dfs.namenode.heartbeat.recheck-interval\n",
    "\n",
    "# get dfs.heartbeat.interval\n",
    "hdfs getconf -confKey dfs.heartbeat.interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# simulate node fault\n",
    "ssh hadoop1 'kill -9 $(cat /tmp/hadoop-hadoop-datanode.pid)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 202384056320 (188.48 GB)\n",
      "Present Capacity: 172184134227 (160.36 GB)\n",
      "DFS Remaining: 171854503936 (160.05 GB)\n",
      "DFS Used: 329630291 (314.36 MB)\n",
      "DFS Used%: 0.19%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (2):\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 164804514 (157.17 MB)\n",
      "Non DFS Used: 9929287774 (9.25 GB)\n",
      "DFS Remaining: 85927251968 (80.03 GB)\n",
      "DFS Used%: 0.16%\n",
      "DFS Remaining%: 84.92%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 18:03:32 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:40:22 GMT 2021\n",
      "Num of Blocks: 34\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 164825777 (157.19 MB)\n",
      "Non DFS Used: 9929266511 (9.25 GB)\n",
      "DFS Remaining: 85927251968 (80.03 GB)\n",
      "DFS Used%: 0.16%\n",
      "DFS Remaining%: 84.92%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 18:03:32 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:46:45 GMT 2021\n",
      "Num of Blocks: 34\n",
      "\n",
      "\n",
      "Dead datanodes (1):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 89298015 (85.16 MB)\n",
      "Non DFS Used: 9915546529 (9.23 GB)\n",
      "DFS Remaining: 86016499712 (80.11 GB)\n",
      "DFS Used%: 0.09%\n",
      "DFS Remaining%: 85.00%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Thu Jan 28 18:01:54 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:44:27 GMT 2021\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Restart nodemanager\n",
    "ssh hadoop1 /opt/hadoop/bin/hdfs --daemon start datanode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 303576084480 (282.73 GB)\n",
      "Present Capacity: 258378976560 (240.63 GB)\n",
      "DFS Remaining: 258049179648 (240.33 GB)\n",
      "DFS Used: 329796912 (314.52 MB)\n",
      "DFS Used%: 0.13%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 89231360 (85.10 MB)\n",
      "Non DFS Used: 9915719680 (9.23 GB)\n",
      "DFS Remaining: 86016393216 (80.11 GB)\n",
      "DFS Used%: 0.09%\n",
      "DFS Remaining%: 85.00%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 18:04:17 GMT 2021\n",
      "Last Block Report: Thu Jan 28 18:03:59 GMT 2021\n",
      "Num of Blocks: 15\n",
      "\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 129303887 (123.31 MB)\n",
      "Non DFS Used: 9875647153 (9.20 GB)\n",
      "DFS Remaining: 86016393216 (80.11 GB)\n",
      "DFS Used%: 0.13%\n",
      "DFS Remaining%: 85.00%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 18:04:17 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:40:22 GMT 2021\n",
      "Num of Blocks: 26\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 101192028160 (94.24 GB)\n",
      "DFS Used: 111261665 (106.11 MB)\n",
      "Non DFS Used: 9893689375 (9.21 GB)\n",
      "DFS Remaining: 86016393216 (80.11 GB)\n",
      "DFS Used%: 0.11%\n",
      "DFS Remaining%: 85.00%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Thu Jan 28 18:04:17 GMT 2021\n",
      "Last Block Report: Thu Jan 28 14:46:45 GMT 2021\n",
      "Num of Blocks: 27\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
