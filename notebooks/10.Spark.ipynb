{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "![Spark](https://spark.apache.org/images/spark-logo-trademark.png)\n",
    "\n",
    "- https://spark.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- version 3.0.1 (Pre-built for Apache Hadoop 3.2 and later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download package\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
    "\n",
    "# unpack file and create link\n",
    "tar -zxf spark-3.0.1-bin-hadoop3.2.tgz -C /opt\n",
    "ln -s /opt/spark-3.0.1-bin-hadoop3.2 /opt/spark\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Spark\n",
    "export SPARK_HOME=/opt/spark\n",
    "export PYSPARK_PYTHON=python3\n",
    "export PYSPARK_DRIVER_PYTHON=ipython3\n",
    "export PYTHONIOENCODING=utf8\n",
    "export PATH=\\${PATH}:\\${SPARK_HOME}/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv -o /opt/envvars.sh\n",
    "%env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Local execution\n",
    "$SPARK_HOME/bin/run-example --master yarn SparkPi 10 2> /dev/null\n",
    "\n",
    "# Local execution with 4 processes\n",
    "# $SPARK_HOME/bin/run-example --master local[4] SparkPi 10\n",
    "\n",
    "# Execution using YARN\n",
    "# $SPARK_HOME/bin/run-example --master yarn SparkPi 10\n",
    "\n",
    "# Execution using spark-submit\n",
    "# $SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi --master local \\\n",
    "# $SPARK_HOME/examples/jars/spark-examples_2.12-3.0.1.jar 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark\n",
    "\n",
    "```bash\n",
    "source /opt/envvars.sh\n",
    "pyspark --master yarn\n",
    "```\n",
    "\n",
    "- Spark application UI - http://localhost:4040\n",
    "\n",
    "```python\n",
    "text_file = sc.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs:///user/hadoop/shakespeare_result\")\n",
    "counts.collect()\n",
    "```\n",
    "\n",
    "```python\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark with Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip3 install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"pi\")\n",
    "conf.setMaster('yarn')\n",
    "conf.set('spark.yarn.dist.files','file:/opt/spark/python/lib/pyspark.zip,file:/opt/spark/python/lib/py4j-0.10.9-src.zip')\n",
    "conf.setExecutorEnv('PYTHONPATH','pyspark.zip:py4j-0.10.9-src.zip')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "num_samples = 10000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"wordcount\")\n",
    "conf.setMaster('yarn')\n",
    "conf.set('spark.yarn.dist.files','file:/opt/spark/python/lib/pyspark.zip,file:/opt/spark/python/lib/py4j-0.10.9-src.zip')\n",
    "conf.setExecutorEnv('PYTHONPATH','pyspark.zip:py4j-0.10.9-src.zip')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "text_file = sc.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "output = counts.take(10)\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark-pictures\n",
    "\n",
    "- https://github.com/jkthompson/pyspark-pictures/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
