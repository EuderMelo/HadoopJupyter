{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "![Spark](https://spark.apache.org/images/spark-logo-trademark.png)\n",
    "\n",
    "- https://spark.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- version 3.0.1 (Pre-built for Apache Hadoop 3.2 and later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
      "export PDSH_RCMD_TYPE=ssh\n",
      "\n",
      "export HADOOP_HOME=/opt/hadoop\n",
      "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
      "export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
      "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
      "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
      "export HADOOP_YARN_HOME=${HADOOP_HOME}\n",
      "\n",
      "export PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin     \n",
      "\n",
      "# Flume\n",
      "export FLUME_HOME=/opt/flume\n",
      "export PATH=${PATH}:${FLUME_HOME}/bin\n",
      "\n",
      "# Sqoop\n",
      "export SQOOP_HOME=/opt/sqoop\n",
      "export PATH=${PATH}:${SQOOP_HOME}/bin\n",
      "\n",
      "# Pig\n",
      "export PIG_HOME=/opt/pig\n",
      "export PATH=${PATH}:${PIG_HOME}/bin\n",
      "\n",
      "# Hive\n",
      "export HIVE_HOME=/opt/hive\n",
      "export PATH=${PATH}:${HIVE_HOME}/bin\n",
      "\n",
      "# Spark\n",
      "export SPARK_HOME=/opt/spark\n",
      "export PYSPARK_PYTHON=python3\n",
      "export PYSPARK_DRIVER_PYTHON=ipython3\n",
      "export PYTHONIOENCODING=utf8\n",
      "export PATH=${PATH}:${SPARK_HOME}/bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Download package\n",
    "cd /opt/pkgs\n",
    "wget -q -c https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
    "\n",
    "# unpack file and create link\n",
    "tar -zxf spark-3.0.1-bin-hadoop3.2.tgz -C /opt\n",
    "ln -s /opt/spark-3.0.1-bin-hadoop3.2 /opt/spark\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Spark\n",
    "export SPARK_HOME=/opt/spark\n",
    "export PYSPARK_PYTHON=python3\n",
    "export PYSPARK_DRIVER_PYTHON=python3\n",
    "export PYTHONIOENCODING=utf8\n",
    "export PATH=\\${PATH}:\\${SPARK_HOME}/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "cat /opt/envvars.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HOSTNAME': 'hadoop',\n",
       " 'OLDPWD': '/',\n",
       " 'PWD': '/opt',\n",
       " 'HOME': '/home/hadoop',\n",
       " 'SHELL': '/bin/bash',\n",
       " 'SHLVL': '1',\n",
       " 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin:/opt/hadoop/sbin:/opt/flume/bin:/opt/sqoop/bin:/opt/pig/bin:/opt/hive/bin:/opt/spark/bin',\n",
       " '_': '/usr/bin/nohup',\n",
       " 'LANGUAGE': 'en.UTF-8',\n",
       " 'LANG': 'en.UTF-8',\n",
       " 'JPY_PARENT_PID': '1566',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
       " 'JAVA_HOME': '/usr/lib/jvm/java-1.8.0-openjdk-amd64',\n",
       " 'PDSH_RCMD_TYPE': 'ssh',\n",
       " 'HADOOP_HOME': '/opt/hadoop',\n",
       " 'HADOOP_COMMON_HOME': '/opt/hadoop',\n",
       " 'HADOOP_CONF_DIR': '/opt/hadoop/etc/hadoop',\n",
       " 'HADOOP_HDFS_HOME': '/opt/hadoop',\n",
       " 'HADOOP_MAPRED_HOME': '/opt/hadoop',\n",
       " 'HADOOP_YARN_HOME': '/opt/hadoop',\n",
       " 'FLUME_HOME': '/opt/flume',\n",
       " 'SQOOP_HOME': '/opt/sqoop',\n",
       " 'PIG_HOME': '/opt/pig',\n",
       " 'HIVE_HOME': '/opt/hive',\n",
       " 'SPARK_HOME': '/opt/spark',\n",
       " 'PYSPARK_PYTHON': 'python3',\n",
       " 'PYSPARK_DRIVER_PYTHON': 'ipython3',\n",
       " 'PYTHONIOENCODING': 'utf8'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv -o /opt/envvars.sh\n",
    "%env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.1416071416071416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-29 17:59:46,559 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-01-29 17:59:47,267 INFO spark.SparkContext: Running Spark version 3.0.1\n",
      "2021-01-29 17:59:47,467 INFO resource.ResourceUtils: ==============================================================\n",
      "2021-01-29 17:59:47,471 INFO resource.ResourceUtils: Resources for spark.driver:\n",
      "\n",
      "2021-01-29 17:59:47,472 INFO resource.ResourceUtils: ==============================================================\n",
      "2021-01-29 17:59:47,475 INFO spark.SparkContext: Submitted application: Spark Pi\n",
      "2021-01-29 17:59:47,745 INFO spark.SecurityManager: Changing view acls to: hadoop\n",
      "2021-01-29 17:59:47,746 INFO spark.SecurityManager: Changing modify acls to: hadoop\n",
      "2021-01-29 17:59:47,747 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2021-01-29 17:59:47,748 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2021-01-29 17:59:47,749 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()\n",
      "2021-01-29 17:59:48,534 INFO util.Utils: Successfully started service 'sparkDriver' on port 41185.\n",
      "2021-01-29 17:59:48,627 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2021-01-29 17:59:48,728 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2021-01-29 17:59:48,810 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2021-01-29 17:59:48,812 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2021-01-29 17:59:48,893 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2021-01-29 17:59:48,931 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-a007be68-9e30-47fe-aae8-726cf947dea3\n",
      "2021-01-29 17:59:48,995 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "2021-01-29 17:59:49,112 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2021-01-29 17:59:49,411 INFO util.log: Logging initialized @6838ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2021-01-29 17:59:49,882 INFO server.Server: jetty-9.4.z-SNAPSHOT; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01\n",
      "2021-01-29 17:59:50,011 INFO server.Server: Started @7443ms\n",
      "2021-01-29 17:59:50,221 INFO server.AbstractConnector: Started ServerConnector@2ef8a8c3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2021-01-29 17:59:50,222 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2021-01-29 17:59:50,423 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f5c79a6{/jobs,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,436 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b46a8c1{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,441 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29caf222{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,493 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f40a43{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,500 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69c43e48{/stages,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,503 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a80515c{/stages/json,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,506 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c807b1d{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,515 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21680803{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,521 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c8b96ec{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,524 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d8f2f3a{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,530 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7048f722{/storage,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,537 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58a55449{/storage/json,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,541 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e0ff644{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,554 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2bb0eb{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,557 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d0566ba{/environment,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,563 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7728643a{/environment/json,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,570 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5167268{/executors,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,572 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28c0b664{/executors/json,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,576 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1af7f54a{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,598 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@436390f4{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,654 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68ed96ca{/static,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,659 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3153ddfc{/,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,662 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28a2a3e7{/api,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,664 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e72dba7{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,670 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1dfd5f51{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2021-01-29 17:59:50,684 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop:4040\n",
      "2021-01-29 17:59:50,746 INFO spark.SparkContext: Added JAR file:/opt/spark/examples/jars/spark-examples_2.12-3.0.1.jar at spark://hadoop:41185/jars/spark-examples_2.12-3.0.1.jar with timestamp 1611943190745\n",
      "2021-01-29 17:59:54,615 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-29 17:59:56,737 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-01-29 17:59:57,007 INFO yarn.Client: Requesting a new application from cluster with 3 NodeManagers\n",
      "2021-01-29 17:59:59,598 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-01-29 17:59:59,599 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-01-29 17:59:59,666 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (1536 MB per container)\n",
      "2021-01-29 17:59:59,668 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "2021-01-29 17:59:59,669 INFO yarn.Client: Setting up container launch context for our AM\n",
      "2021-01-29 17:59:59,670 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "2021-01-29 17:59:59,697 INFO yarn.Client: Preparing resources for our AM container\n",
      "2021-01-29 17:59:59,775 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "2021-01-29 18:00:07,243 INFO yarn.Client: Uploading resource file:/tmp/spark-84899979-fdc3-48c9-8d2f-47bd3b7029a2/__spark_libs__7516141809034719277.zip -> hdfs://hadoop:9000/user/hadoop/.sparkStaging/application_1611844877680_0021/__spark_libs__7516141809034719277.zip\n",
      "2021-01-29 18:00:18,315 INFO yarn.Client: Uploading resource file:/tmp/spark-84899979-fdc3-48c9-8d2f-47bd3b7029a2/__spark_conf__6591934747327934921.zip -> hdfs://hadoop:9000/user/hadoop/.sparkStaging/application_1611844877680_0021/__spark_conf__.zip\n",
      "2021-01-29 18:00:18,684 INFO spark.SecurityManager: Changing view acls to: hadoop\n",
      "2021-01-29 18:00:18,684 INFO spark.SecurityManager: Changing modify acls to: hadoop\n",
      "2021-01-29 18:00:18,685 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2021-01-29 18:00:18,685 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2021-01-29 18:00:18,686 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()\n",
      "2021-01-29 18:00:18,753 INFO yarn.Client: Submitting application application_1611844877680_0021 to ResourceManager\n",
      "2021-01-29 18:00:18,948 INFO impl.YarnClientImpl: Submitted application application_1611844877680_0021\n",
      "2021-01-29 18:00:20,061 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:20,094 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Fri Jan 29 18:00:19 +0000 2021] Scheduler has assigned a container for AM, waiting for AM container to be launched\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1611943218814\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://hadoop:8088/proxy/application_1611844877680_0021/\n",
      "\t user: hadoop\n",
      "2021-01-29 18:00:21,118 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:22,126 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:23,130 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:24,141 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:25,160 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:26,166 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:27,181 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:28,190 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:29,205 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:30,211 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:31,224 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:32,195 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:33,200 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:34,231 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:35,246 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:36,271 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:37,285 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:38,291 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:39,301 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:40,309 INFO yarn.Client: Application report for application_1611844877680_0021 (state: ACCEPTED)\n",
      "2021-01-29 18:00:41,320 INFO yarn.Client: Application report for application_1611844877680_0021 (state: RUNNING)\n",
      "2021-01-29 18:00:41,334 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.17.0.4\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1611943218814\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://hadoop:8088/proxy/application_1611844877680_0021/\n",
      "\t user: hadoop\n",
      "2021-01-29 18:00:41,337 INFO cluster.YarnClientSchedulerBackend: Application application_1611844877680_0021 has started running.\n",
      "2021-01-29 18:00:41,377 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40215.\n",
      "2021-01-29 18:00:41,379 INFO netty.NettyBlockTransferService: Server created on hadoop:40215\n",
      "2021-01-29 18:00:41,398 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2021-01-29 18:00:41,436 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop, 40215, None)\n",
      "2021-01-29 18:00:41,455 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop:40215 with 366.3 MiB RAM, BlockManagerId(driver, hadoop, 40215, None)\n",
      "2021-01-29 18:00:41,483 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop, 40215, None)\n",
      "2021-01-29 18:00:41,490 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop, 40215, None)\n",
      "2021-01-29 18:00:41,610 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6f798482{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2021-01-29 18:00:41,880 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
      "2021-01-29 18:00:43,228 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hadoop, PROXY_URI_BASES -> http://hadoop:8088/proxy/application_1611844877680_0021), /proxy/application_1611844877680_0021\n",
      "2021-01-29 18:00:48,282 INFO spark.SparkContext: Starting job: reduce at SparkPi.scala:38\n",
      "2021-01-29 18:00:48,711 INFO scheduler.DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 10 output partitions\n",
      "2021-01-29 18:00:48,713 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n",
      "2021-01-29 18:00:48,718 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2021-01-29 18:00:48,744 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2021-01-29 18:00:48,867 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n",
      "2021-01-29 18:00:51,007 INFO scheduler.AsyncEventQueue: Process of event SparkListenerJobStart(0,1611943248745,WrappedArray(org.apache.spark.scheduler.StageInfo@2f6504f3),{spark.rdd.scope={\"id\":\"2\",\"name\":\"reduce\"}, spark.rdd.scope.noOverride=true}) by listener AppStatusListener took 2.1163936s.\n",
      "2021-01-29 18:00:51,524 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.1 KiB, free 366.3 MiB)\n",
      "2021-01-29 18:00:52,357 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1816.0 B, free 366.3 MiB)\n",
      "2021-01-29 18:00:52,441 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop:40215 (size: 1816.0 B, free: 366.3 MiB)\n",
      "2021-01-29 18:00:52,533 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223\n",
      "2021-01-29 18:00:52,848 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\n",
      "2021-01-29 18:00:52,918 INFO cluster.YarnScheduler: Adding task set 0.0 with 10 tasks\n",
      "2021-01-29 18:00:53,017 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "2021-01-29 18:01:08,357 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2021-01-29 18:01:23,308 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "2021-01-29 18:01:25,048 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2021-01-29 18:01:29,708 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.5:40056) with ID 2\n",
      "2021-01-29 18:01:30,051 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.0.3:54780) with ID 1\n",
      "2021-01-29 18:01:30,894 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop3:34223 with 366.3 MiB RAM, BlockManagerId(2, hadoop3, 34223, None)\n",
      "2021-01-29 18:01:30,904 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop1:36155 with 366.3 MiB RAM, BlockManagerId(1, hadoop1, 36155, None)\n",
      "2021-01-29 18:01:31,167 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hadoop3, executor 2, partition 0, PROCESS_LOCAL, 7404 bytes)\n",
      "2021-01-29 18:01:31,253 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hadoop1, executor 1, partition 1, PROCESS_LOCAL, 7404 bytes)\n",
      "2021-01-29 18:01:34,576 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop1:36155 (size: 1816.0 B, free: 366.3 MiB)\n",
      "2021-01-29 18:01:34,771 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop3:34223 (size: 1816.0 B, free: 366.3 MiB)\n",
      "2021-01-29 18:01:44,041 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, hadoop1, executor 1, partition 2, PROCESS_LOCAL, 7404 bytes)\n",
      "2021-01-29 18:01:44,314 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 13064 ms on hadoop1 (executor 1) (1/10)\n",
      "2021-01-29 18:01:44,631 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, hadoop3, executor 2, partition 3, PROCESS_LOCAL, 7404 bytes)\n",
      "2021-01-29 18:01:44,682 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 13588 ms on hadoop3 (executor 2) (2/10)\n",
      "2021-01-29 18:01:44,760 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, hadoop1, executor 1, partition 4, PROCESS_LOCAL, 7404 bytes)\n",
      "2021-01-29 18:01:44,788 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 748 ms on hadoop1 (executor 1) (3/10)\n",
      "2021-01-29 18:01:45,050 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, hadoop3, executor 2, partition 5, PROCESS_LOCAL, 7404 bytes)\n",
      "2021-01-29 18:01:45,059 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 300 ms on hadoop1 (executor 1) (4/10)\n",
      "2021-01-29 18:01:45,082 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, hadoop1, executor 1, partition 6, PROCESS_LOCAL, 7404 bytes)\n",
      "2021-01-29 18:01:45,094 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 465 ms on hadoop3 (executor 2) (5/10)\n",
      "2021-01-29 18:01:45,313 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, hadoop3, executor 2, partition 7, PROCESS_LOCAL, 7404 bytes)\n",
      "2021-01-29 18:01:45,315 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 267 ms on hadoop3 (executor 2) (6/10)\n",
      "2021-01-29 18:01:45,394 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 324 ms on hadoop1 (executor 1) (7/10)\n",
      "2021-01-29 18:01:45,424 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, hadoop1, executor 1, partition 8, PROCESS_LOCAL, 7404 bytes)\n",
      "2021-01-29 18:01:45,504 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, hadoop3, executor 2, partition 9, PROCESS_LOCAL, 7404 bytes)\n",
      "2021-01-29 18:01:45,514 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 217 ms on hadoop3 (executor 2) (8/10)\n",
      "2021-01-29 18:01:45,639 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 233 ms on hadoop1 (executor 1) (9/10)\n",
      "2021-01-29 18:01:45,646 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 143 ms on hadoop3 (executor 2) (10/10)\n",
      "2021-01-29 18:01:45,660 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 56.040 s\n",
      "2021-01-29 18:01:45,681 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2021-01-29 18:01:45,774 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2021-01-29 18:01:45,785 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\n",
      "2021-01-29 18:01:45,907 INFO scheduler.DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 57.692383 s\n",
      "2021-01-29 18:01:46,099 INFO server.AbstractConnector: Stopped Spark@2ef8a8c3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "2021-01-29 18:01:46,115 INFO ui.SparkUI: Stopped Spark web UI at http://hadoop:4040\n",
      "2021-01-29 18:01:46,182 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\n",
      "2021-01-29 18:01:46,473 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\n",
      "2021-01-29 18:01:46,494 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n",
      "2021-01-29 18:01:46,549 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\n",
      "2021-01-29 18:01:47,498 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2021-01-29 18:01:47,626 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2021-01-29 18:01:47,627 INFO storage.BlockManager: BlockManager stopped\n",
      "2021-01-29 18:01:47,736 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2021-01-29 18:01:47,767 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2021-01-29 18:01:48,035 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "2021-01-29 18:01:48,194 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "2021-01-29 18:01:48,214 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-6c37ed77-0e79-41bc-90fc-580dc72037ad\n",
      "2021-01-29 18:01:48,548 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-84899979-fdc3-48c9-8d2f-47bd3b7029a2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Local execution\n",
    "#$SPARK_HOME/bin/run-example --master local SparkPi 10 2> /dev/null\n",
    "\n",
    "# Local execution with 4 processes\n",
    "#$SPARK_HOME/bin/run-example --master local[4] SparkPi 10\n",
    "\n",
    "# Execution using YARN\n",
    "# $SPARK_HOME/bin/run-example --master yarn SparkPi 10\n",
    "\n",
    "# Execution using spark-submit\n",
    "$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn \\\n",
    " $SPARK_HOME/examples/jars/spark-examples_2.12-3.0.1.jar 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark\n",
    "\n",
    "```bash\n",
    "source /opt/envvars.sh\n",
    "pyspark --master yarn\n",
    "```\n",
    "\n",
    "- Spark application UI - http://localhost:4040\n",
    "\n",
    "```python\n",
    "text_file = sc.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs:///user/hadoop/shakespeare_result\")\n",
    "counts.collect()\n",
    "```\n",
    "\n",
    "```python\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark-pictures\n",
    "\n",
    "- https://github.com/jkthompson/pyspark-pictures/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
