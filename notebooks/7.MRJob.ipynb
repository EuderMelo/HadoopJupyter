{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HOSTNAME': 'hadoop',\n",
       " 'OLDPWD': '/',\n",
       " 'PWD': '/opt',\n",
       " 'HOME': '/home/hadoop',\n",
       " 'SHELL': '/bin/bash',\n",
       " 'SHLVL': '1',\n",
       " 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin:/opt/hadoop/sbin:/opt/flume/bin:/opt/sqoop/bin',\n",
       " '_': '/usr/bin/nohup',\n",
       " 'LANGUAGE': 'en.UTF-8',\n",
       " 'LANG': 'en.UTF-8',\n",
       " 'JPY_PARENT_PID': '1566',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
       " 'JAVA_HOME': '/usr/lib/jvm/java-1.8.0-openjdk-amd64',\n",
       " 'PDSH_RCMD_TYPE': 'ssh',\n",
       " 'HADOOP_HOME': '/opt/hadoop',\n",
       " 'HADOOP_COMMON_HOME': '/opt/hadoop',\n",
       " 'HADOOP_CONF_DIR': '/opt/hadoop/etc/hadoop',\n",
       " 'HADOOP_HDFS_HOME': '/opt/hadoop',\n",
       " 'HADOOP_MAPRED_HOME': '/opt/hadoop',\n",
       " 'HADOOP_YARN_HOME': '/opt/hadoop',\n",
       " 'FLUME_HOME': '/opt/flume',\n",
       " 'SQOOP_HOME': '/opt/sqoop'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "%load_ext dotenv\n",
    "%dotenv -o /opt/envvars.sh\n",
    "%env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRJob\n",
    "\n",
    "- https://github.com/Yelp/mrjob\n",
    "- https://mrjob.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mrjob\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/58/fc28ab743aba16e90736ad4e29694bd2adaf7b879376ff149306d50c4e90/mrjob-0.7.4-py2.py3-none-any.whl (439kB)\n",
      "Collecting PyYAML>=3.10 (from mrjob)\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/5b/bc0b5ab38247bba158504a410112b6c03f153c652734ece1849749e5f518/PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640kB)\n",
      "Installing collected packages: PyYAML, mrjob\n",
      "Successfully installed PyYAML-5.4.1 mrjob-0.7.4\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# local install\n",
    "pip3 install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/src/mrjob\n"
     ]
    }
   ],
   "source": [
    "%mkdir /opt/src/mrjob\n",
    "%cd /opt/src/mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mrwordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrwordcount.py\n",
    "import re\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            word = word.lower()\n",
    "            yield word,1\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/datasets\n",
    "# download book \"The History of Don Quixote by Miguel de Cervantes\" from Gutenberg Project\n",
    "wget -q -c http://www.gutenberg.org/files/996/996-0.txt -O donquixote.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"lackeys\"\t1\n",
      "\"lacking\"\t1\n",
      "\"lacquered\"\t1\n",
      "\"lacquey\"\t26\n",
      "\"lacqueys\"\t1\n",
      "\"lad\"\t16\n",
      "\"ladder\"\t1\n",
      "\"ladders\"\t1\n",
      "\"laden\"\t4\n",
      "\"ladies\"\t84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for local runner\n",
      "Creating temp directory /tmp/mrwordcount.hadoop.20210129.130828.810038\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/mrwordcount.hadoop.20210129.130828.810038/output\n",
      "Streaming final output from /tmp/mrwordcount.hadoop.20210129.130828.810038/output...\n",
      "Removing temp directory /tmp/mrwordcount.hadoop.20210129.130828.810038...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# inline runner (default)\n",
    "#python3 mrwordcount.py /opt/datasets/donquixote.txt > /opt/datasets/donquixote-output.txt\n",
    "\n",
    "# local runner\n",
    "python3 mrwordcount.py -r local /opt/datasets/donquixote.txt > /opt/datasets/donquixote-output.txt\n",
    "\n",
    "# head output\n",
    "head /opt/datasets/donquixote-output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop3: Collecting mrjob\n",
      "hadoop2: Collecting mrjob\n",
      "hadoop1: Collecting mrjob\n",
      "hadoop3:   Downloading https://files.pythonhosted.org/packages/8e/58/fc28ab743aba16e90736ad4e29694bd2adaf7b879376ff149306d50c4e90/mrjob-0.7.4-py2.py3-none-any.whl (439kB)\n",
      "hadoop2:   Downloading https://files.pythonhosted.org/packages/8e/58/fc28ab743aba16e90736ad4e29694bd2adaf7b879376ff149306d50c4e90/mrjob-0.7.4-py2.py3-none-any.whl (439kB)\n",
      "hadoop1:   Downloading https://files.pythonhosted.org/packages/8e/58/fc28ab743aba16e90736ad4e29694bd2adaf7b879376ff149306d50c4e90/mrjob-0.7.4-py2.py3-none-any.whl (439kB)\n",
      "hadoop2: Collecting PyYAML>=3.10 (from mrjob)\n",
      "hadoop1: Collecting PyYAML>=3.10 (from mrjob)\n",
      "hadoop3: Collecting PyYAML>=3.10 (from mrjob)\n",
      "hadoop3:   Downloading https://files.pythonhosted.org/packages/7a/5b/bc0b5ab38247bba158504a410112b6c03f153c652734ece1849749e5f518/PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640kB)\n",
      "hadoop1:   Downloading https://files.pythonhosted.org/packages/7a/5b/bc0b5ab38247bba158504a410112b6c03f153c652734ece1849749e5f518/PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640kB)\n",
      "hadoop2:   Downloading https://files.pythonhosted.org/packages/7a/5b/bc0b5ab38247bba158504a410112b6c03f153c652734ece1849749e5f518/PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640kB)\n",
      "hadoop3: Installing collected packages: PyYAML, mrjob\n",
      "hadoop2: Installing collected packages: PyYAML, mrjob\n",
      "hadoop1: Installing collected packages: PyYAML, mrjob\n",
      "hadoop3: Successfully installed PyYAML-5.4.1 mrjob-0.7.4\n",
      "hadoop2: Successfully installed PyYAML-5.4.1 mrjob-0.7.4\n",
      "hadoop1: Successfully installed PyYAML-5.4.1 mrjob-0.7.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hadoop1: Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n",
      "hadoop3: Warning: Permanently added 'hadoop3,172.17.0.5' (ECDSA) to the list of known hosts.\n",
      "hadoop2: Warning: Permanently added 'hadoop2,172.17.0.4' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# install in all hadoop nodes\n",
    "pdsh -w hadoop1,hadoop2,hadoop3 pip3 install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-29 13:11:10,925 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.2.1\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar\n",
      "Creating temp directory /tmp/mrwordcount.hadoop.20210129.131112.308929\n",
      "uploading working dir files to hdfs:///user/hadoop/tmp/mrjob/mrwordcount.hadoop.20210129.131112.308929/files/wd...\n",
      "Copying other local files to hdfs:///user/hadoop/tmp/mrjob/mrwordcount.hadoop.20210129.131112.308929/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar3235736449570638627/] [] /tmp/streamjob8909491657222742448.jar tmpDir=null\n",
      "  Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "  Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "  Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "  Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1611844877680_0009\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Total input files to process : 1\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  number of splits:2\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Submitting tokens for job: job_1611844877680_0009\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1611844877680_0009\n",
      "  The url to track the job: http://hadoop:8088/proxy/application_1611844877680_0009/\n",
      "  Running job: job_1611844877680_0009\n",
      "  Job job_1611844877680_0009 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 37% reduce 0%\n",
      "   map 67% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1611844877680_0009 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/donquixote-output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2394958\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=204012\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=340735\n",
      "\t\tFILE: Number of bytes written=1374205\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2395174\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=204012\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20131328\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3616256\n",
      "\t\tTotal time spent by all map tasks (ms)=78638\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=157276\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14126\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=28252\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=78638\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=14126\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=40270\n",
      "\t\tCombine input records=435439\n",
      "\t\tCombine output records=23909\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=913\n",
      "\t\tInput split bytes=216\n",
      "\t\tMap input records=43281\n",
      "\t\tMap output bytes=3990544\n",
      "\t\tMap output materialized bytes=340741\n",
      "\t\tMap output records=435439\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=289943552\n",
      "\t\tPeak Map Virtual memory (bytes)=1980674048\n",
      "\t\tPeak Reduce Physical memory (bytes)=155762688\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1918713856\n",
      "\t\tPhysical memory (bytes) snapshot=690933760\n",
      "\t\tReduce input groups=16444\n",
      "\t\tReduce input records=23909\n",
      "\t\tReduce output records=16444\n",
      "\t\tReduce shuffle bytes=340741\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=47818\n",
      "\t\tTotal committed heap usage (bytes)=467664896\n",
      "\t\tVirtual memory (bytes) snapshot=5743345664\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/hadoop/donquixote-output\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/mrwordcount.hadoop.20210129.131112.308929...\n",
      "Removing temp directory /tmp/mrwordcount.hadoop.20210129.131112.308929...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# create directory in HDFS and send file\n",
    "hdfs dfs -mkdir donquixote\n",
    "hdfs dfs -put /opt/datasets/donquixote.txt donquixote\n",
    "\n",
    "# run in hadoop\n",
    "python3 mrwordcount.py -r hadoop --output-dir donquixote-output hdfs:///user/hadoop/donquixote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0\"\t2\n",
      "\"000\"\t1\n",
      "\"1\"\t45\n",
      "\"104k\"\t1\n",
      "\"105k\"\t1\n",
      "\"106k\"\t1\n",
      "\"1085\"\t1\n",
      "\"108k\"\t1\n",
      "\"109k\"\t2\n",
      "\"10k\"\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-29 13:14:14,597 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# get output\n",
    "hdfs dfs -getmerge donquixote-output donquixote-output.txt\n",
    "\n",
    "# head output\n",
    "head donquixote-output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "- weblog.csv\n",
    "- books from Gutenberg project\n",
    "- departments.csv and employees.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd /opt/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "tar -zxf mrjobdataset.tgz -C /opt/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/src/mrjob\n"
     ]
    }
   ],
   "source": [
    "%cd /opt/src/mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1_count_weblog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 1_count_weblog.py\n",
    "#Total number of times each page is visited\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRURLCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "\n",
    "        #Extract site\n",
    "        url = request.split(' ')[1]\n",
    "\n",
    "        #Emit url and 1\n",
    "        yield url, 1\n",
    "\n",
    "    def reducer(self, key, list_of_values):\n",
    "        yield key,sum(list_of_values)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRURLCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"/img/ruet.png\"\t213\n",
      "\"/index.php\"\t6\n",
      "\"/js/chart.min.js\"\t58\n",
      "\"/js/jquery.min.js\"\t56\n",
      "\"/js/vendor/jquery-1.12.0.min.js\"\t387\n",
      "\"/js/vendor/modernizr-2.8.3.min.js\"\t1417\n",
      "\"/js/vendor/moment.min.js\"\t173\n",
      "\"/login.php\"\t3298\n",
      "\"/login.php?value=fail\"\t128\n",
      "\"/logout.php\"\t44\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 1_count_weblog.py /opt/datasets/weblog.csv 2> /dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2_max_weblog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2_max_weblog.py\n",
    "# Return most visited URL\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRURLMax(MRJob) :\n",
    "\n",
    "    def mapper1(self, _, line) :\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "\n",
    "        #Extract site\n",
    "        url = request.split(' ')[1]\n",
    "\n",
    "        #Emit url and 1\n",
    "        yield url, 1\n",
    "\n",
    "    def reducer1(self, key, list_of_values) :\n",
    "        yield None, (sum(list_of_values), key)\n",
    "\n",
    "    def reducer2(self, key, list_of_values) :\n",
    "        yield max(list_of_values)\n",
    "\n",
    "    def steps(self) :\n",
    "        return [MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "        MRStep(reducer=self.reducer2)]\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    MRURLMax.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3298\t\"/login.php\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 2_max_weblog.py /opt/datasets/weblog.csv 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_average_weblog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_average_weblog.py\n",
    "#Average visit time\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRAvgVisitTime(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = float(data[4].strip())\n",
    "\n",
    "        #Extract site\n",
    "        url = request.split(' ')[1]\n",
    "\n",
    "        #Emit url and visit time\n",
    "        yield url, visit\n",
    "\n",
    "    def reducer(self, key, list_of_values):\n",
    "        count = 0\n",
    "        total = 0.0\n",
    "        for x in list_of_values:\n",
    "            total = total + x\n",
    "            count = count + 1\n",
    "\n",
    "        avglen = (\"%.2f\" % (total/count))\n",
    "        yield key,avglen\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRAvgVisitTime.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"/img/ruet.png\"\t\"258.97\"\n",
      "\"/index.php\"\t\"227.32\"\n",
      "\"/js/chart.min.js\"\t\"299.16\"\n",
      "\"/js/jquery.min.js\"\t\"207.62\"\n",
      "\"/js/vendor/jquery-1.12.0.min.js\"\t\"252.07\"\n",
      "\"/js/vendor/modernizr-2.8.3.min.js\"\t\"244.30\"\n",
      "\"/js/vendor/moment.min.js\"\t\"279.61\"\n",
      "\"/login.php\"\t\"252.70\"\n",
      "\"/login.php?value=fail\"\t\"227.98\"\n",
      "\"/logout.php\"\t\"237.21\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 3_average_weblog.py /opt/datasets/weblog.csv 2> /dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 4_topn_weblog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 4_topn_weblog.py\n",
    "#Top 3 visited pages\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRTopN(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "\n",
    "        #Extract url\n",
    "        url = request.split(' ')[1]\n",
    "\n",
    "        #Emit url and 1\n",
    "        yield url, 1\n",
    "\n",
    "    def reducer1(self, key, list_of_values):\n",
    "        total_count = sum(list_of_values)\n",
    "        yield None, (total_count, key)\n",
    "\n",
    "    def reducer2(self, _, list_of_values):\n",
    "        N=3\n",
    "        list_of_values = sorted(list(list_of_values), reverse=True)\n",
    "        return list_of_values[:N]\n",
    "\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper, reducer=self.reducer1),\n",
    "        MRStep(reducer=self.reducer2)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRTopN.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3298\t\"/login.php\"\n",
      "2653\t\"/home.php\"\n",
      "1417\t\"/js/vendor/modernizr-2.8.3.min.js\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 4_topn_weblog.py /opt/datasets/weblog.csv 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 5_filter_weblog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 5_filter_weblog.py\n",
    "#Filter accesses to \"/login.php?value=fail\" on Feb/2018\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRFilter(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "\n",
    "        #Extract site\n",
    "        url = request.split(' ')[1]\n",
    "\n",
    "        #Extract month/year\n",
    "        date = time[4:12]\n",
    "\n",
    "        #Filter access to \"/login.php?value=fail\" on Feb/2018\n",
    "        if url == \"/login.php?value=fail\" and date == \"Feb/2018\" :\n",
    "            yield url, (time, ip, visit)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRFilter.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"/login.php?value=fail\"\t[\"[17/Feb/2018:20:08:56\", \"10.128.2.1\", \"185.3492762625749\"]\n",
      "\"/login.php?value=fail\"\t[\"[17/Feb/2018:20:20:28\", \"10.130.2.1\", \"231.88065750818035\"]\n",
      "\"/login.php?value=fail\"\t[\"[18/Feb/2018:19:40:31\", \"10.128.2.1\", \"119.2142303257265\"]\n",
      "\"/login.php?value=fail\"\t[\"[18/Feb/2018:19:40:35\", \"10.128.2.1\", \"147.46020967961022\"]\n",
      "\"/login.php?value=fail\"\t[\"[18/Feb/2018:19:40:38\", \"10.131.0.1\", \"257.82366658285235\"]\n",
      "\"/login.php?value=fail\"\t[\"[18/Feb/2018:19:40:39\", \"10.131.0.1\", \"169.11496206962957\"]\n",
      "\"/login.php?value=fail\"\t[\"[20/Feb/2018:11:14:24\", \"10.130.2.1\", \"330.90262403457416\"]\n",
      "\"/login.php?value=fail\"\t[\"[22/Feb/2018:06:26:02\", \"10.131.0.1\", \"766.0191400216718\"]\n",
      "\"/login.php?value=fail\"\t[\"[22/Feb/2018:06:26:08\", \"10.128.2.1\", \"34.905177651134075\"]\n",
      "\"/login.php?value=fail\"\t[\"[22/Feb/2018:06:26:10\", \"10.128.2.1\", \"49.17624847339196\"]\n",
      "\"/login.php?value=fail\"\t[\"[22/Feb/2018:06:26:11\", \"10.128.2.1\", \"268.1915691580601\"]\n",
      "\"/login.php?value=fail\"\t[\"[22/Feb/2018:06:26:33\", \"10.130.2.1\", \"68.96655453836624\"]\n",
      "\"/login.php?value=fail\"\t[\"[22/Feb/2018:06:26:38\", \"10.130.2.1\", \"35.34159176564747\"]\n",
      "\"/login.php?value=fail\"\t[\"[22/Feb/2018:06:30:04\", \"10.130.2.1\", \"132.7902384183176\"]\n",
      "\"/login.php?value=fail\"\t[\"[22/Feb/2018:06:30:08\", \"10.130.2.1\", \"181.43019268284462\"]\n",
      "\"/login.php?value=fail\"\t[\"[22/Feb/2018:06:30:13\", \"10.130.2.1\", \"140.19336438252802\"]\n",
      "\"/login.php?value=fail\"\t[\"[22/Feb/2018:12:43:05\", \"10.128.2.1\", \"191.28279631322528\"]\n",
      "\"/login.php?value=fail\"\t[\"[22/Feb/2018:12:43:13\", \"10.128.2.1\", \"54.3434972757474\"]\n",
      "\"/login.php?value=fail\"\t[\"[25/Feb/2018:13:34:22\", \"10.128.2.1\", \"385.5021656353063\"]\n",
      "\"/login.php?value=fail\"\t[\"[25/Feb/2018:17:25:11\", \"10.128.2.1\", \"36.92692421159084\"]\n",
      "\"/login.php?value=fail\"\t[\"[26/Feb/2018:03:55:24\", \"10.130.2.1\", \"390.67091933717063\"]\n",
      "\"/login.php?value=fail\"\t[\"[26/Feb/2018:03:55:56\", \"10.131.0.1\", \"113.72912981897535\"]\n",
      "\"/login.php?value=fail\"\t[\"[26/Feb/2018:03:56:01\", \"10.131.0.1\", \"180.1077939718322\"]\n",
      "\"/login.php?value=fail\"\t[\"[26/Feb/2018:03:56:03\", \"10.131.0.1\", \"60.35491691867254\"]\n",
      "\"/login.php?value=fail\"\t[\"[26/Feb/2018:03:56:04\", \"10.131.0.1\", \"305.96401522218554\"]\n",
      "\"/login.php?value=fail\"\t[\"[28/Feb/2018:03:44:51\", \"10.130.2.1\", \"106.17974660973063\"]\n",
      "\"/login.php?value=fail\"\t[\"[28/Feb/2018:20:47:32\", \"10.130.2.1\", \"555.7294064467495\"]\n",
      "\"/login.php?value=fail\"\t[\"[28/Feb/2018:20:47:50\", \"10.131.0.1\", \"26.838604094485987\"]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 5_filter_weblog.py /opt/datasets/weblog.csv 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 6_distinct_weblog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 6_distinct_weblog.py\n",
    "#Distinct IPs\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRDistinct(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "\n",
    "        yield ip, None\n",
    "\n",
    "    def reducer(self, key, list_of_values) :\n",
    "        yield key, None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRDistinct.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"10.131.2.1\"\tnull\n",
      "\"10.128.2.1\"\tnull\n",
      "\"10.131.0.1\"\tnull\n",
      "\"10.129.2.1\"\tnull\n",
      "\"10.130.2.1\"\tnull\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 6_distinct_weblog.py /opt/datasets/weblog.csv 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 7_binning_weblog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 7_binning_weblog.py\n",
    "#Create bins for different status codes for 20/Feb/2018\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRBinning(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "        \n",
    "        #Extract month/year\n",
    "        date = time[1:12]\n",
    "\n",
    "        #Filter accesses on 20/Feb/2018\n",
    "        if date == \"20/Feb/2018\" :\n",
    "            yield status, (time, request, ip)\n",
    "\n",
    "    def reducer(self, key, list_of_values):\n",
    "        yield key, (list(list_of_values))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRBinning.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"200\"\t[[\"[20/Feb/2018:01:51:41\", \"GET /login.php HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:07:06:12\", \"GET /login.php HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:09:23:17\", \"GET /login.php HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:09:23:21\", \"GET /fonts/fontawesome-webfont.woff2?v=4.6.3 HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:09:23:29\", \"GET /home.php HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:09:23:30\", \"GET /bootstrap-3.3.7/js/bootstrap.js HTTP/1.1\", \"10.131.0.1\"], [\"[20/Feb/2018:09:24:12\", \"GET /compiler.php HTTP/1.1\", \"10.131.0.1\"], [\"[20/Feb/2018:11:04:47\", \"GET /login.php HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:11:04:51\", \"GET /css/bootstrap.min.css HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:11:04:52\", \"GET /css/normalize.css HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:11:04:55\", \"GET /css/font-awesome.min.css HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:11:04:57\", \"GET /css/main.css HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:11:04:58\", \"GET /css/style.css HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:11:13:54\", \"GET /login.php HTTP/1.1\", \"10.131.0.1\"], [\"[20/Feb/2018:11:13:55\", \"GET /css/bootstrap.min.css HTTP/1.1\", \"10.131.0.1\"], [\"[20/Feb/2018:11:13:55\", \"GET /css/font-awesome.min.css HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:11:13:55\", \"GET /css/normalize.css HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:11:13:55\", \"GET /css/main.css HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:11:13:56\", \"GET /css/style.css HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:11:13:56\", \"GET /js/vendor/modernizr-2.8.3.min.js HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:11:13:56\", \"GET /js/vendor/jquery-1.12.0.min.js HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:11:13:56\", \"GET /bootstrap-3.3.7/js/bootstrap.min.js HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:11:14:24\", \"GET /login.php?value=fail HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:13:34:44\", \"GET /login.php HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:13:58:18\", \"GET /login.php HTTP/1.1\", \"10.131.0.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /login.php HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /css/font-awesome.min.css HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /css/bootstrap.min.css HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /js/vendor/jquery-1.12.0.min.js HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /css/style.css HTTP/1.1\", \"10.131.0.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /bootstrap-3.3.7/js/bootstrap.min.js HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /css/main.css HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /js/vendor/modernizr-2.8.3.min.js HTTP/1.1\", \"10.131.0.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /js/vendor/modernizr-2.8.3.min.js HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /css/normalize.css HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /img/ruet.png HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /css/normalize.css HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:14:38:25\", \"GET /fonts/fontawesome-webfont.woff?v=4.6.3 HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:16:09:54\", \"GET /login.php HTTP/1.1\", \"10.130.2.1\"]]\n",
      "\"404\"\t[[\"[20/Feb/2018:01:48:55\", \"GET /robots.txt HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:13:58:09\", \"GET /robots.txt HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:16:08:45\", \"GET /robots.txt HTTP/1.1\", \"10.130.2.1\"]]\n",
      "\"302\"\t[[\"[20/Feb/2018:01:50:40\", \"GET / HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:07:05:37\", \"GET / HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:09:23:17\", \"GET / HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:09:23:29\", \"POST /process.php HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:11:04:14\", \"GET / HTTP/1.1\", \"10.131.0.1\"], [\"[20/Feb/2018:11:13:54\", \"GET / HTTP/1.1\", \"10.131.0.1\"], [\"[20/Feb/2018:11:14:24\", \"POST /process.php HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:13:34:15\", \"GET / HTTP/1.1\", \"10.131.0.1\"], [\"[20/Feb/2018:13:58:13\", \"GET / HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:14:38:25\", \"GET / HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:16:09:18\", \"GET / HTTP/1.1\", \"10.130.2.1\"]]\n",
      "\"304\"\t[[\"[20/Feb/2018:09:23:18\", \"GET /css/bootstrap.min.css HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:09:23:18\", \"GET /css/font-awesome.min.css HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:09:23:18\", \"GET /css/normalize.css HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:09:23:18\", \"GET /css/main.css HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:09:23:18\", \"GET /css/style.css HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:09:23:18\", \"GET /js/vendor/modernizr-2.8.3.min.js HTTP/1.1\", \"10.131.0.1\"], [\"[20/Feb/2018:09:23:18\", \"GET /js/vendor/jquery-1.12.0.min.js HTTP/1.1\", \"10.128.2.1\"], [\"[20/Feb/2018:09:23:19\", \"GET /bootstrap-3.3.7/js/bootstrap.min.js HTTP/1.1\", \"10.130.2.1\"], [\"[20/Feb/2018:09:23:31\", \"GET /js/vendor/moment.min.js HTTP/1.1\", \"10.128.2.1\"]]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 7_binning_weblog.py /opt/datasets/weblog.csv 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 8_invertedindex_books.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 8_invertedindex_books.py\n",
    "#Inverted Index\n",
    "from mrjob.job import MRJob\n",
    "import os\n",
    "\n",
    "class MRInvertedIndex(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        fileName = os.environ['mapreduce_map_input_file']\n",
    "\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            yield word, fileName\n",
    "\n",
    "    def reducer(self, key, list_of_values):\n",
    "        docs = set()\n",
    "        for x in list_of_values :\n",
    "            docs.add(x)\n",
    "        yield key,list(docs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRInvertedIndex.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf /opt/datasets/books/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"life,\"\t[\"file:///opt/datasets/books/book1.txt\", \"file:///opt/datasets/books/book2.txt\", \"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life,\\u2014all\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life,\\u2014in\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life--and\"\t[\"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life--animal\"\t[\"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life--was\"\t[\"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life-blood\"\t[\"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life-boats.\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-buoy\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-buoy-coffin\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-buoy\\u2014a\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-buoys\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-giving,\"\t[\"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life-like\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-like,\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-line,\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-lines,\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-long\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-preserver\\u2014an\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-preservers.\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-principle\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-restless\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-spot\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-time\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life-work,\"\t[\"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life.\"\t[\"file:///opt/datasets/books/book1.txt\", \"file:///opt/datasets/books/book2.txt\", \"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life.'\"\t[\"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life.\\u201d\"\t[\"file:///opt/datasets/books/book1.txt\"]\n",
      "\"life:--\"\t[\"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life;\"\t[\"file:///opt/datasets/books/book1.txt\", \"file:///opt/datasets/books/book2.txt\", \"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life?\"\t[\"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life?\\\"\"\t[\"file:///opt/datasets/books/book3.txt\"]\n",
      "\"life\\u2014I\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life\\u2014\\u2014\\u201d\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life\\u2014\\u201d\"\t[\"file:///opt/datasets/books/book1.txt\"]\n",
      "\"life\\u2014as\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"life\\u2019s\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"lifeless\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"lifelessly\"\t[\"file:///opt/datasets/books/book2.txt\"]\n",
      "\"lifetime\"\t[\"file:///opt/datasets/books/book2.txt\"]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 8_invertedindex_books.py /opt/datasets/books 2> /dev/null | head -n 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 9_sort_weblog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 9_sort_weblog.py\n",
    "# Sort visit times in descending order\n",
    "from mrjob.job import MRJob\n",
    "class MRSortVisit(MRJob) :\n",
    "    def mapper(self, _, line):\n",
    "        #Split the line with comma separated fields\n",
    "        data = line.split(',')\n",
    "\n",
    "        #Parse line\n",
    "        ip = data[0].strip()\n",
    "        #Check if it's not the header line\n",
    "        if ip == 'IP' : return\n",
    "        time = data[1].strip()\n",
    "        request = data[2].strip()\n",
    "        status = data[3].strip()\n",
    "        visit = data[4].strip()\n",
    "\n",
    "        #Extract site\n",
    "        url = request.split(' ')[1]\n",
    "\n",
    "        yield None, (visit, (time, url, ip))\n",
    "\n",
    "    def reducer(self, key, list_of_values):\n",
    "        l = [(float(v), content) for v, content in list_of_values]\n",
    "        l.sort(reverse=True)\n",
    "        return l\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRSortVisit.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8742.057189992775\t[\"[29/Jan/2018:20:33:45\", \"/login.php\", \"10.128.2.1\"]\n",
      "6287.629575490433\t[\"[29/Jan/2018:20:55:42\", \"/login.php\", \"10.128.2.1\"]\n",
      "5711.326270236046\t[\"[29/Jan/2018:20:35:56\", \"/js/vendor/modernizr-2.8.3.min.js\", \"10.128.2.1\"]\n",
      "5283.59276598012\t[\"[09/Nov/2017:19:53:43\", \"/js/vendor/modernizr-2.8.3.min.js\", \"10.131.0.1\"]\n",
      "5123.493680523937\t[\"[29/Jan/2018:20:47:45\", \"/js/vendor/modernizr-2.8.3.min.js\", \"10.128.2.1\"]\n",
      "5053.873518356491\t[\"[13/Nov/2017:09:11:14\", \"/login.php\", \"10.131.2.1\"]\n",
      "4642.003763780086\t[\"[29/Jan/2018:20:29:30\", \"/js/vendor/modernizr-2.8.3.min.js\", \"10.131.0.1\"]\n",
      "4615.5689039456365\t[\"[29/Jan/2018:20:32:40\", \"/login.php\", \"10.128.2.1\"]\n",
      "4421.817090530212\t[\"[29/Jan/2018:20:35:37\", \"/js/vendor/modernizr-2.8.3.min.js\", \"10.131.0.1\"]\n",
      "3857.8085677731\t[\"[24/Nov/2017:08:29:49\", \"/css/font-awesome.min.css\", \"10.131.2.1\"]\n",
      "3725.3713482396056\t[\"[29/Jan/2018:20:34:22\", \"/login.php\", \"10.131.0.1\"]\n",
      "3667.904220839362\t[\"[12/Nov/2017:19:44:01\", \"/archive.php\", \"10.130.2.1\"]\n",
      "3570.29271074942\t[\"[29/Jan/2018:20:29:30\", \"/login.php\", \"10.131.0.1\"]\n",
      "3485.9893149824265\t[\"[29/Jan/2018:20:35:33\", \"/login.php\", \"10.131.0.1\"]\n",
      "3473.156110992622\t[\"[23/Nov/2017:18:35:44\", \"/css/main.css\", \"10.128.2.1\"]\n",
      "3471.1054516357603\t[\"[30/Nov/2017:19:47:30\", \"/robots.txt\", \"10.131.0.1\"]\n",
      "3356.568134399352\t[\"[14/Dec/2017:11:13:28\", \"/css/font-awesome.min.css\", \"10.131.0.1\"]\n",
      "3214.708818153326\t[\"[29/Jan/2018:20:22:42\", \"/css/bootstrap.min.css\", \"10.130.2.1\"]\n",
      "3207.5925234924525\t[\"[30/Nov/2017:13:25:50\", \"/archive.php?page=2\", \"10.129.2.1\"]\n",
      "3202.8604736680045\t[\"[29/Nov/2017:17:55:01\", \"/css/style.css\", \"10.129.2.1\"]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 9_sort_weblog.py /opt/datasets/weblog.csv 2> /dev/null | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InnerJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 10_innerjoin_db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 10_innerjoin_db.py\n",
    "from mrjob.job import MRJob\n",
    "import os\n",
    "\n",
    "class MRInnerJoin(MRJob) :\n",
    "    def mapper(self, _, line):\n",
    "        data = line.split(',')\n",
    "\n",
    "        filename = os.environ['mapreduce_map_input_file']\n",
    "\n",
    "        if 'employees.csv' in filename :\n",
    "            dep_no = data[2]\n",
    "            yield dep_no, ('Employee', data)\n",
    "        elif 'departments.csv' in filename:\n",
    "            dep_no = data[0]\n",
    "            yield dep_no, ('Department', data)\n",
    "\n",
    "    def reducer(self, key, list_of_values) :\n",
    "        values = list(list_of_values)\n",
    "        employees = []\n",
    "        departments = []\n",
    "        for v in values:\n",
    "            if v[0] == 'Employee' :\n",
    "                employees.append(v)\n",
    "            elif v[0] == 'Department' :\n",
    "                departments.append(v)\n",
    "\n",
    "        # Inner Join\n",
    "        for e in employees :\n",
    "            for d in departments :\n",
    "                yield key, (e+d)\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    MRInnerJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"d006\"\t[\"Employee\", [\"10009\", \"Sumant Peac\", \"d006\", \"1985-02-18\"], \"Department\", [\"d006\", \"Quality Management\"]]\n",
      "\"d006\"\t[\"Employee\", [\"10010\", \"Duangkaew Piveteau\", \"d006\", \"1989-08-24\"], \"Department\", [\"d006\", \"Quality Management\"]]\n",
      "\"d006\"\t[\"Employee\", [\"10029\", \"Otmar Herbst\", \"d006\", \"1985-11-20\"], \"Department\", [\"d006\", \"Quality Management\"]]\n",
      "\"d006\"\t[\"Employee\", [\"10033\", \"Arif Merlo\", \"d006\", \"1987-03-18\"], \"Department\", [\"d006\", \"Quality Management\"]]\n",
      "\"d006\"\t[\"Employee\", [\"10067\", \"Claudi Stavenow\", \"d006\", \"1987-03-04\"], \"Department\", [\"d006\", \"Quality Management\"]]\n",
      "\"d006\"\t[\"Employee\", [\"10073\", \"Shir McClurg\", \"d006\", \"1991-12-01\"], \"Department\", [\"d006\", \"Quality Management\"]]\n",
      "\"d006\"\t[\"Employee\", [\"10111\", \"Hugo Rosis\", \"d006\", \"1988-06-19\"], \"Department\", [\"d006\", \"Quality Management\"]]\n",
      "\"d006\"\t[\"Employee\", [\"10124\", \"Geraldo Marwedel\", \"d006\", \"1991-09-05\"], \"Department\", [\"d006\", \"Quality Management\"]]\n",
      "\"d006\"\t[\"Employee\", [\"10138\", \"Perry Shimshoni\", \"d006\", \"1986-09-18\"], \"Department\", [\"d006\", \"Quality Management\"]]\n",
      "\"d006\"\t[\"Employee\", [\"10152\", \"Jaques Munro\", \"d006\", \"1986-01-27\"], \"Department\", [\"d006\", \"Quality Management\"]]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python3 10_innerjoin_db.py /opt/datasets/employees.csv /opt/datasets/departments.csv 2> /dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeftOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 11_leftouterjoin_db.py\n",
    "from mrjob.job import MRJob\n",
    "import os\n",
    "\n",
    "class MRLeftOuterJoin(MRJob) :\n",
    "    def mapper(self, _, line):\n",
    "        data = line.split(',')\n",
    "\n",
    "        filename = os.environ['mapreduce_map_input_file']\n",
    "\n",
    "        if 'employees.csv' in filename :\n",
    "            dep_no = data[2]\n",
    "            yield dep_no, ('Employee', data)\n",
    "        elif 'departments.csv' in filename:\n",
    "            dep_no = data[0]\n",
    "            yield dep_no, ('Department', data)\n",
    "\n",
    "    def reducer(self, key, list_of_values) :\n",
    "        # yield None, list(list_of_values)\n",
    "        values = list(list_of_values)\n",
    "        employees = []\n",
    "        departments = []\n",
    "        for v in values:\n",
    "            if v[0] == 'Employee' :\n",
    "                employees.append(v)\n",
    "            elif v[0] == 'Department' :\n",
    "                departments.append(v)\n",
    "\n",
    "        # Left Outer Join\n",
    "        for e in employees :\n",
    "            if len(departments) > 0 :\n",
    "                for d in departments :\n",
    "                    yield key, (e+d)\n",
    "            else :\n",
    "                yield key, (e)\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    MRLeftOuterJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python3 11_leftouterjoin_db.py /opt/datasets/employees.csv /opt/datasets/departments.csv 2> /dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RightOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 12_rightouterjoin_db.py\n",
    "from mrjob.job import MRJob\n",
    "import os\n",
    "\n",
    "class MRRightOuterJoin(MRJob) :\n",
    "    def mapper(self, _, line):\n",
    "        data = line.split(',')\n",
    "\n",
    "        filename = os.environ['mapreduce_map_input_file']\n",
    "        \n",
    "        if 'employees.csv' in filename :\n",
    "            dep_no = data[2]\n",
    "            yield dep_no, ('Employee', data)\n",
    "        elif 'departments.csv' in filename:\n",
    "            dep_no = data[0]\n",
    "            yield dep_no, ('Department', data)\n",
    "\n",
    "    def reducer(self, key, list_of_values) :\n",
    "        # yield None, list(list_of_values)\n",
    "        values = list(list_of_values)\n",
    "        employees = []\n",
    "        departments = []\n",
    "        for v in values:\n",
    "            if v[0] == 'Employee' :\n",
    "                employees.append(v)\n",
    "            elif v[0] == 'Department' :\n",
    "                departments.append(v)\n",
    "\n",
    "        # Right Outer Join\n",
    "        for d in departments :\n",
    "            if len(employees) > 0 :\n",
    "                for e in employees :\n",
    "                    yield key, (e+d)\n",
    "            else :\n",
    "                yield key, (d)\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    MRRightOuterJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python3 12_rightouterjoin_db.py /opt/datasets/employees.csv /opt/datasets/departments.csv 2> /dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FullOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 13_fullouterjoin_db.py\n",
    "from mrjob.job import MRJob\n",
    "import os\n",
    "\n",
    "class MRFullOuterJoin(MRJob) :\n",
    "    def mapper(self, _, line):\n",
    "        data = line.split(',')\n",
    "\n",
    "        filename = os.environ['mapreduce_map_input_file']\n",
    "\n",
    "        if 'employees.csv' in filename :\n",
    "            dep_no = data[2]\n",
    "            yield dep_no, ('Employee', data)\n",
    "        elif 'departments.csv' in filename:\n",
    "            dep_no = data[0]\n",
    "            yield dep_no, ('Department', data)\n",
    "\n",
    "    def reducer(self, key, list_of_values) :\n",
    "        values = list(list_of_values)\n",
    "        employees = []\n",
    "        departments = []\n",
    "        for v in values:\n",
    "            if v[0] == 'Employee' :\n",
    "                employees.append(v)\n",
    "            elif v[0] == 'Department' :\n",
    "                departments.append(v)\n",
    "\n",
    "        # Full Outer Join\n",
    "        if len(employees) > 0 :\n",
    "            for e in employees :\n",
    "                if len(departments) > 0 :\n",
    "                    for d in departments :\n",
    "                        yield key, (e+d)\n",
    "                else :\n",
    "                    yield key, (e)\n",
    "        else :\n",
    "            yield None, (d)\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    MRFullOuterJoin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python3 13_fullouterjoin_db.py /opt/datasets/employees.csv /opt/datasets/departments.csv 2> /dev/null | head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
