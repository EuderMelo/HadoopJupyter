{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading https://files.pythonhosted.org/packages/32/2e/e4585559237787966aad0f8fd0fc31df1c4c9eb0e62de458c5b6cde954eb/python_dotenv-0.15.0-py2.py3-none-any.whl\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-0.15.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'HOSTNAME': 'hadoop',\n",
       " 'OLDPWD': '/',\n",
       " 'PWD': '/opt',\n",
       " 'HOME': '/home/hadoop',\n",
       " 'SHELL': '/bin/bash',\n",
       " 'SHLVL': '1',\n",
       " 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/hadoop/bin:/opt/hadoop/sbin',\n",
       " '_': '/usr/bin/nohup',\n",
       " 'LANGUAGE': 'en.UTF-8',\n",
       " 'LANG': 'en.UTF-8',\n",
       " 'JPY_PARENT_PID': '1566',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
       " 'JAVA_HOME': '/usr/lib/jvm/java-1.8.0-openjdk-amd64',\n",
       " 'PDSH_RCMD_TYPE': 'ssh',\n",
       " 'HADOOP_HOME': '/opt/hadoop',\n",
       " 'HADOOP_COMMON_HOME': '/opt/hadoop',\n",
       " 'HADOOP_CONF_DIR': '/opt/hadoop/etc/hadoop',\n",
       " 'HADOOP_HDFS_HOME': '/opt/hadoop',\n",
       " 'HADOOP_MAPRED_HOME': '/opt/hadoop',\n",
       " 'HADOOP_YARN_HOME': '/opt/hadoop'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source /opt/envvars.sh\n",
    "\n",
    "!pip3 install python-dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv -o /opt/envvars.sh\n",
    "%env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN - Web interface\n",
    "\n",
    "- Master node\n",
    "    - Resource Manager: http://localhost:8088\n",
    "    - Timeline Service: http://localhost:8188\n",
    "- Worker node\n",
    "    - hadoop1\n",
    "        - NodeManager: http://localhost:8042\n",
    "    - hadoop2\n",
    "        - NodeManager: http://localhost:8043\n",
    "    - hadoop3\n",
    "        - NodeManager: http://localhost:8044"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop MapReduce Examples\n",
    "\n",
    "```\n",
    "$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar\n",
    "```\n",
    "\n",
    "- aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n",
    "- aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n",
    "- bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n",
    "- dbcount: An example job that count the pageview counts from a database.\n",
    "- distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n",
    "- grep: A map/reduce program that counts the matches of a regex in the input.\n",
    "- join: A job that effects a join over sorted, equally partitioned datasets\n",
    "- multifilewc: A job that counts words from several files.\n",
    "- pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n",
    "- pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n",
    "- randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n",
    "- randomwriter: A map/reduce program that writes 10GB of random data per node.\n",
    "- secondarysort: An example defining a secondary sort to the reduce.\n",
    "- sort: A map/reduce program that sorts the data written by the random writer.\n",
    "- sudoku: A sudoku solver.\n",
    "- teragen: Generate data for the terasort\n",
    "- terasort: Run the terasort\n",
    "- teravalidate: Checking results of terasort\n",
    "- wordcount: A map/reduce program that counts the words in the input files.\n",
    "- wordmean: A map/reduce program that counts the average length of the words in the input files.\n",
    "- wordmedian: A map/reduce program that counts the median length of the words in the input files.\n",
    "- wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 3 maps.\n",
      "Job started: Thu Jan 28 16:19:33 GMT 2021\n",
      "Job ended: Thu Jan 28 16:20:54 GMT 2021\n",
      "The job took 81 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 16:19:30,335 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-28 16:19:31,073 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-01-28 16:19:33,387 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-28 16:19:33,389 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-01-28 16:19:33,847 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1611844877680_0002\n",
      "2021-01-28 16:19:34,159 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 16:19:34,680 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 16:19:34,893 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 16:19:34,964 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2021-01-28 16:19:35,725 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 16:19:35,911 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1611844877680_0002\n",
      "2021-01-28 16:19:35,911 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-01-28 16:19:37,102 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-01-28 16:19:37,104 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-01-28 16:19:37,415 INFO impl.YarnClientImpl: Submitted application application_1611844877680_0002\n",
      "2021-01-28 16:19:38,027 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1611844877680_0002/\n",
      "2021-01-28 16:19:38,037 INFO mapreduce.Job: Running job: job_1611844877680_0002\n",
      "2021-01-28 16:19:59,607 INFO mapreduce.Job: Job job_1611844877680_0002 running in uber mode : false\n",
      "2021-01-28 16:19:59,616 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-01-28 16:20:48,195 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2021-01-28 16:20:49,567 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2021-01-28 16:20:50,653 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-01-28 16:20:54,021 INFO mapreduce.Job: Job job_1611844877680_0002 completed successfully\n",
      "2021-01-28 16:20:54,405 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=677898\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=360\n",
      "\t\tHDFS: Number of bytes written=157767117\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tOther local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=272170\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=136085\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=136085\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=34837760\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3\n",
      "\t\tMap output records=240025\n",
      "\t\tInput split bytes=360\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=2956\n",
      "\t\tCPU time spent (ms)=28370\n",
      "\t\tPhysical memory (bytes) snapshot=517931008\n",
      "\t\tVirtual memory (bytes) snapshot=5746126848\n",
      "\t\tTotal committed heap usage (bytes)=255852544\n",
      "\t\tPeak Map Physical memory (bytes)=174125056\n",
      "\t\tPeak Map Virtual memory (bytes)=1917661184\n",
      "\torg.apache.hadoop.examples.RandomTextWriter$Counters\n",
      "\t\tBYTES_WRITTEN=157287067\n",
      "\t\tRECORDS_WRITTEN=240025\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=157767117\n",
      "2021-01-28 16:21:03,111 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-28 16:21:03,774 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-01-28 16:21:04,484 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1611844877680_0003\n",
      "2021-01-28 16:21:04,812 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 16:21:05,309 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2021-01-28 16:21:05,446 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 16:21:05,579 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 16:21:05,650 INFO mapreduce.JobSubmitter: number of splits:6\n",
      "2021-01-28 16:21:06,053 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-28 16:21:06,158 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1611844877680_0003\n",
      "2021-01-28 16:21:06,159 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-01-28 16:21:06,769 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-01-28 16:21:06,770 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-01-28 16:21:07,073 INFO impl.YarnClientImpl: Submitted application application_1611844877680_0003\n",
      "2021-01-28 16:21:07,290 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1611844877680_0003/\n",
      "2021-01-28 16:21:07,294 INFO mapreduce.Job: Running job: job_1611844877680_0003\n",
      "2021-01-28 16:21:30,758 INFO mapreduce.Job: Job job_1611844877680_0003 running in uber mode : false\n",
      "2021-01-28 16:21:30,768 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-01-28 16:22:19,685 INFO mapreduce.Job:  map 16% reduce 0%\n",
      "2021-01-28 16:22:25,901 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2021-01-28 16:22:33,629 INFO mapreduce.Job:  map 28% reduce 0%\n",
      "2021-01-28 16:22:49,111 INFO mapreduce.Job:  map 31% reduce 0%\n",
      "2021-01-28 16:22:50,259 INFO mapreduce.Job:  map 35% reduce 0%\n",
      "2021-01-28 16:22:54,663 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "2021-01-28 16:22:55,679 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "2021-01-28 16:22:56,696 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "2021-01-28 16:22:58,745 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2021-01-28 16:22:59,782 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "2021-01-28 16:23:01,865 INFO mapreduce.Job:  map 58% reduce 0%\n",
      "2021-01-28 16:23:02,885 INFO mapreduce.Job:  map 69% reduce 0%\n",
      "2021-01-28 16:23:05,945 INFO mapreduce.Job:  map 74% reduce 0%\n",
      "2021-01-28 16:23:09,007 INFO mapreduce.Job:  map 77% reduce 0%\n",
      "2021-01-28 16:23:12,143 INFO mapreduce.Job:  map 78% reduce 0%\n",
      "2021-01-28 16:23:18,216 INFO mapreduce.Job:  map 78% reduce 11%\n",
      "2021-01-28 16:23:20,260 INFO mapreduce.Job:  map 83% reduce 11%\n",
      "2021-01-28 16:23:22,728 INFO mapreduce.Job:  map 89% reduce 11%\n",
      "2021-01-28 16:23:23,744 INFO mapreduce.Job:  map 89% reduce 22%\n",
      "2021-01-28 16:23:33,121 INFO mapreduce.Job:  map 94% reduce 22%\n",
      "2021-01-28 16:23:34,167 INFO mapreduce.Job:  map 100% reduce 22%\n",
      "2021-01-28 16:23:36,198 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-01-28 16:23:37,233 INFO mapreduce.Job: Job job_1611844877680_0003 completed successfully\n",
      "2021-01-28 16:23:37,485 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=249867\n",
      "\t\tFILE: Number of bytes written=1982898\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=157780119\n",
      "\t\tHDFS: Number of bytes written=16655\n",
      "\t\tHDFS: Number of read operations=23\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=2\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=8\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1353432\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=118372\n",
      "\t\tTotal time spent by all map tasks (ms)=676716\n",
      "\t\tTotal time spent by all reduce tasks (ms)=59186\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=676716\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=59186\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=173239296\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=15151616\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=240025\n",
      "\t\tMap output records=14761317\n",
      "\t\tMap output bytes=216332335\n",
      "\t\tMap output materialized bytes=149931\n",
      "\t\tInput split bytes=714\n",
      "\t\tCombine input records=14761317\n",
      "\t\tCombine output records=9000\n",
      "\t\tReduce input groups=1000\n",
      "\t\tReduce shuffle bytes=149931\n",
      "\t\tReduce input records=9000\n",
      "\t\tReduce output records=1000\n",
      "\t\tSpilled Records=24000\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=16120\n",
      "\t\tCPU time spent (ms)=134710\n",
      "\t\tPhysical memory (bytes) snapshot=1839386624\n",
      "\t\tVirtual memory (bytes) snapshot=13395877888\n",
      "\t\tTotal committed heap usage (bytes)=1331691520\n",
      "\t\tPeak Map Physical memory (bytes)=286507008\n",
      "\t\tPeak Map Virtual memory (bytes)=1916022784\n",
      "\t\tPeak Reduce Physical memory (bytes)=168976384\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1918603264\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=157779405\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=16655\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# hadoop jar ./hadoop-mapreduce-examples-3.2.1.jar\n",
    "\n",
    "# randomwriter: A map/reduce program that writes 10GB of random data per node\n",
    "# configured for 150MB of random text / 50 MB per map (3 maps)\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.1.jar randomtextwriter \\\n",
    "  -D mapreduce.randomtextwriter.totalbytes=157286400 \\\n",
    "  -D mapreduce.randomtextwriter.bytespermap=52428800 \\\n",
    "  -D mapreduce.output.fileoutputformat.compress=false \\\n",
    "  -outFormat org.apache.hadoop.mapreduce.lib.output.TextOutputFormat \\\n",
    "  randomtext\n",
    "\n",
    "# wordcount: A map/reduce program that counts the words in the input files\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.1.jar wordcount randomtext randomtextcount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# Teragen - 200MB\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.1.jar teragen \\\n",
    "  2000000 \\\n",
    "  teragenoutput\n",
    "\n",
    "# Terasort\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.1.jar terasort \\\n",
    "  teragenoutput terasortoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN - CLI\n",
    "\n",
    "- https://hadoop.apache.org/docs/r3.2.1/hadoop-yarn/hadoop-yarn-site/YarnCommands.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: yarn [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
      " or    yarn [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
      "  where CLASSNAME is a user-provided Java class\n",
      "\n",
      "  OPTIONS is none or any of:\n",
      "\n",
      "--buildpaths                       attempt to add class files from build tree\n",
      "--config dir                       Hadoop config directory\n",
      "--daemon (start|status|stop)       operate on a daemon\n",
      "--debug                            turn on shell script debug mode\n",
      "--help                             usage information\n",
      "--hostnames list[,of,host,names]   hosts to use in worker mode\n",
      "--hosts filename                   list of hosts to use in worker mode\n",
      "--loglevel level                   set the log4j level for this command\n",
      "--workers                          turn on worker mode\n",
      "\n",
      "  SUBCOMMAND is one of:\n",
      "\n",
      "\n",
      "    Admin Commands:\n",
      "\n",
      "daemonlog            get/set the log level for each daemon\n",
      "node                 prints node report(s)\n",
      "rmadmin              admin tools\n",
      "scmadmin             SharedCacheManager admin tools\n",
      "\n",
      "    Client Commands:\n",
      "\n",
      "applicationattempt   prints applicationattempt(s) report\n",
      "app|application      prints application(s) report/kill application/manage\n",
      "                     long running application\n",
      "classpath            prints the class path needed to get the hadoop jar and\n",
      "                     the required libraries\n",
      "cluster              prints cluster information\n",
      "container            prints container(s) report\n",
      "envvars              display computed Hadoop environment variables\n",
      "jar <jar>            run a jar file\n",
      "logs                 dump container logs\n",
      "nodeattributes       node attributes cli client\n",
      "queue                prints queue information\n",
      "schedulerconf        Updates scheduler configuration\n",
      "timelinereader       run the timeline reader server\n",
      "top                  view cluster information\n",
      "version              print the version\n",
      "\n",
      "    Daemon Commands:\n",
      "\n",
      "nodemanager          run a nodemanager on each worker\n",
      "proxyserver          run the web app proxy server\n",
      "registrydns          run the registry DNS server\n",
      "resourcemanager      run the ResourceManager\n",
      "router               run the Router daemon\n",
      "sharedcachemanager   run the SharedCacheManager daemon\n",
      "timelineserver       run the timeline server\n",
      "\n",
      "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "yarn help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List cluster node status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Nodes:3\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "   hadoop2:37641\t        RUNNING\t     hadoop2:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1536, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:4896 MB, VMem:4911 MB, VCores:1.4285715\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n",
      "   hadoop3:46557\t        RUNNING\t     hadoop3:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1536, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:4896 MB, VMem:4911 MB, VCores:1.3886081\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n",
      "   hadoop1:40677\t        RUNNING\t     hadoop1:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1536, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:4896 MB, VMem:4911 MB, VCores:1.4256825\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 16:26:14,555 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-28 16:26:15,360 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-01-28 16:26:15,927 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-01-28 16:26:15,929 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "yarn node -all -list -showDetails\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of applications (application-types: [], states: [NEW, NEW_SAVING, SUBMITTED, ACCEPTED, RUNNING, FINISHED, FAILED, KILLED] and tags: []):3\n",
      "                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\n",
      "application_1611844877680_0001\t     QuasiMonteCarlo\t           MAPREDUCE\t    hadoop\t   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://hadoop1:19888/jobhistory/job/job_1611844877680_0001\n",
      "application_1611844877680_0002\t  random-text-writer\t           MAPREDUCE\t    hadoop\t   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://hadoop1:19888/jobhistory/job/job_1611844877680_0002\n",
      "application_1611844877680_0003\t          word count\t           MAPREDUCE\t    hadoop\t   default\t          FINISHED\t         SUCCEEDED\t           100%\thttp://hadoop3:19888/jobhistory/job/job_1611844877680_0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 16:27:09,769 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-28 16:27:10,508 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Current applications (running or accepted)\n",
    "# yarn app -list\n",
    "# Applications already executed\n",
    "yarn app -list -appStates ALL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List queue status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queue Information : \n",
      "Queue Name : default\n",
      "\tState : RUNNING\n",
      "\tCapacity : 100.0%\n",
      "\tCurrent Capacity : .0%\n",
      "\tMaximum Capacity : 100.0%\n",
      "\tDefault Node Label expression : <DEFAULT_PARTITION>\n",
      "\tAccessible Node Labels : *\n",
      "\tPreemption : disabled\n",
      "\tIntra-queue Preemption : disabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 16:34:15,217 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-28 16:34:15,949 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "yarn queue -status default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get application log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting logs for application_1611844877680_0003\n",
      "Container: container_1611844877680_0003_01_000006 on hadoop1_40677\n",
      "LogAggregationType: AGGREGATED\n",
      "==================================================================\n",
      "LogType:directory.info\n",
      "LogLastModifiedTime:Thu Jan 28 16:23:42 +0000 2021\n",
      "LogLength:1989\n",
      "LogContents:\n",
      "ls -l:\n",
      "total 32\n",
      "-rw-r--r-- 1 hadoop hadoop  129 Jan 28 16:21 container_tokens\n",
      "-rwx------ 1 hadoop hadoop  733 Jan 28 16:21 default_container_executor.sh\n",
      "-rwx------ 1 hadoop hadoop  678 Jan 28 16:21 default_container_executor_session.sh\n",
      "lrwxrwxrwx 1 hadoop hadoop  109 Jan 28 16:21 job.jar -> /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1611844877680_0003/filecache/10/job.jar\n",
      "lrwxrwxrwx 1 hadoop hadoop  109 Jan 28 16:21 job.xml -> /tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1611844877680_0003/filecache/11/job.xml\n",
      "-rwx------ 1 hadoop hadoop 5060 Jan 28 16:21 launch_container.sh\n",
      "drwx--x--- 2 hadoop hadoop 4096 Jan 28 16:21 tmp\n",
      "find -L . -maxdepth 5 -ls:\n",
      "  2491420      4 drwx--x---   3 hadoop   hadoop       4096 Jan 28 16:21 .\n",
      "  2491435      4 -rw-r--r--   1 hadoop   hadoop         48 Jan 28 16:21 ./.launch_container.sh.crc\n",
      "  2491436      4 -rwx------   1 hadoop   hadoop        678 Jan 28 16:21 ./default_container_executor_session.sh\n",
      "  2491433      8 -rwx------   1 hadoop   hadoop       5060 Jan 28 16:21 ./launch_container.sh\n",
      "  2491439      4 -rw-r--r--   1 hadoop   hadoop         16 Jan 28 16:21 ./.default_container_executor.sh.crc\n",
      "  2491438      4 -rwx------   1 hadoop   hadoop        733 Jan 28 16:21 ./default_container_executor.sh\n",
      "  2491383      4 drwx------   2 hadoop   hadoop       4096 Jan 28 16:21 ./job.jar\n",
      "  2491385    312 -r-x------   1 hadoop   hadoop     316534 Jan 28 16:21 ./job.jar/job.jar\n",
      "  2491376    192 -r-x------   1 hadoop   hadoop     192784 Jan 28 16:21 ./job.xml\n",
      "  2491431      4 -rw-r--r--   1 hadoop   hadoop         12 Jan 28 16:21 ./.container_tokens.crc\n",
      "  2491426      4 -rw-r--r--   1 hadoop   hadoop        129 Jan 28 16:21 ./container_tokens\n",
      "  2491423      4 drwx--x---   2 hadoop   hadoop       4096 Jan 28 16:21 ./tmp\n",
      "  2491437      4 -rw-r--r--   1 hadoop   hadoop         16 Jan 28 16:21 ./.default_container_executor_session.sh.crc\n",
      "broken symlinks(find -L . -maxdepth 5 -type l -ls):\n",
      "\n",
      "End of LogType:directory.info\n",
      "*******************************************************************************\n",
      "\n",
      "Container: container_1611844877680_0003_01_000006 on hadoop1_40677\n",
      "LogAggregationType: AGGREGATED\n",
      "==================================================================\n",
      "LogType:launch_container.sh\n",
      "LogLastModifiedTime:Thu Jan 28 16:23:42 +0000 2021\n",
      "LogLength:5060\n",
      "LogContents:\n",
      "#!/bin/bash\n",
      "\n",
      "set -o pipefail -e\n",
      "export PRELAUNCH_OUT=\"/opt/hadoop-3.2.1/logs/userlogs/application_1611844877680_0003/container_1611844877680_0003_01_000006/prelaunch.out\"\n",
      "exec >\"${PRELAUNCH_OUT}\"\n",
      "export PRELAUNCH_ERR=\"/opt/hadoop-3.2.1/logs/userlogs/application_1611844877680_0003/container_1611844877680_0003_01_000006/prelaunch.err\"\n",
      "exec 2>\"${PRELAUNCH_ERR}\"\n",
      "echo \"Setting up env variables\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "APPID=$(yarn application -list -appStates FINISHED 2>/dev/null | tail -1 | awk '{ print $1 }')\n",
    "echo \"Getting logs for $APPID\"\n",
    "yarn logs -applicationId $APPID 2>/dev/null | head -n 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kill application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "APPID=$(yarn application -list -appStates RUNNING 2>/dev/null | tail -1 | awk '{ print $1 }')\n",
    "echo \"Killing $APPID\"\n",
    "yarn application -kill $APPID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling nodemanager failures\n",
    "\n",
    "- yarn.nm.liveness-monitor.expiry-interval-ms property in yarn-site.xml\n",
    " - set to 10000 (10 seconds) / default is 600000 (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# simulate node fault\n",
    "ssh hadoop1 'kill -9 $(cat /tmp/hadoop-hadoop-nodemanager.pid)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8088/cluster/nodes\n",
    "- Wait 10 seconds to discover node failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Nodes:3\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "   hadoop1:39857\t        RUNNING\t     hadoop1:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1536, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:4774 MB, VMem:4790 MB, VCores:0.93602693\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n",
      "   hadoop2:37641\t        RUNNING\t     hadoop2:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1536, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:4792 MB, VMem:4807 MB, VCores:1.5723906\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n",
      "   hadoop3:46557\t        RUNNING\t     hadoop3:8042\t                           0\n",
      "Detailed Node Information :\n",
      "\tConfigured Resources : <memory:1536, vCores:8>\n",
      "\tAllocated Resources : <memory:0, vCores:0>\n",
      "\tResource Utilization by Node : PMem:4792 MB, VMem:4807 MB, VCores:1.5785931\n",
      "\tResource Utilization by Containers : PMem:0 MB, VMem:0 MB, VCores:0.0\n",
      "\tNode-Labels : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-28 16:41:43,933 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-28 16:41:44,737 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-01-28 16:41:45,158 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-01-28 16:41:45,159 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "yarn node -list -showDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'hadoop1,172.17.0.3' (ECDSA) to the list of known hosts.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Restart nodemanager\n",
    "ssh hadoop1 /opt/hadoop/bin/yarn --daemon start nodemanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
