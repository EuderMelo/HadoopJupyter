{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive\n",
    "![Hive](https://hive.apache.org/images/hive_logo_medium.jpg)\n",
    "\n",
    "- https://hive.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- version 3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download package\n",
    "wget -q -c https://downloads.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz\n",
    "\n",
    "# Copy installation package to container\n",
    "docker cp apache-hive-3.1.2-bin.tar.gz hadoop:/opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "\n",
    "# unpack file and create link\n",
    "tar -zxf /opt/apache-hive-3.1.2-bin.tar.gz -C /opt\n",
    "ln -s /opt/apache-hive-3.1.2-bin /opt/hive\n",
    "\n",
    "# update envvars.sh\n",
    "cat >> /opt/envvars.sh << EOF\n",
    "# Hive\n",
    "export HIVE_HOME=/opt/hive\n",
    "export PATH=\\$PATH:\\$HIVE_HOME/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "# Fix guava and slf4j versions\n",
    "rm /opt/hive/lib/guava-19.0.jar\n",
    "cp /opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar /opt/hive/lib\n",
    "rm /opt/hive/lib/log4j-slf4j-impl-2.10.0.jar\n",
    "\n",
    "sudo rm /opt/apache-hive-3.1.2-bin.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop configuration (for beeline)\n",
    "\n",
    "- core-site.xml\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "...\n",
    "<property>\n",
    "  <name>hadoop.proxyuser.hadoop.groups</name>\n",
    "  <value>*</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>hadoop.proxyuser.hadoop.hosts</name>\n",
    "  <value>*</value>\n",
    "</property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Metastore\n",
    "\n",
    "- using local Derby database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory '/opt/hive/hiveserver2': File exists\n",
      "Metastore connection URL:\t jdbc:derby:;databaseName=metastore_db;create=true\n",
      "Metastore Connection Driver :\t org.apache.derby.jdbc.EmbeddedDriver\n",
      "Metastore connection User:\t APP\n",
      "Starting metastore schema initialization to 3.1.0\n",
      "Initialization script hive-schema-3.1.0.derby.sql\n",
      "\n",
      " \n",
      "Error: FUNCTION 'NUCLEUS_ASCII' already exists. (state=X0Y68,code=30000)\n",
      "org.apache.hadoop.hive.metastore.HiveMetaException: Schema initialization FAILED! Metastore state would be inconsistent !!\n",
      "Underlying cause: java.io.IOException : Schema script failed, errorcode 2\n",
      "Use --verbose for detailed stacktrace.\n",
      "*** schemaTool failed ***\n",
      "2021-01-11 02:42:54: Starting HiveServer2\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# create directory in HDFS\n",
    "hdfs dfs -mkdir -p /user/hive/warehouse\n",
    "hdfs dfs -chmod g+w /user/hive/warehouse\n",
    "\n",
    "# initialize database\n",
    "mkdir $HIVE_HOME/hiveserver2\n",
    "cd $HIVE_HOME/hiveserver2\n",
    "$HIVE_HOME/bin/schematool -dbType derby -initSchema\n",
    "\n",
    "# start server\n",
    "nohup $HIVE_HOME/bin/hive --service hiveserver2 \\\n",
    "--hiveconf hive.security.authorization.createtable.owner.grants=ALL \\\n",
    "--hiveconf hive.root.logger=INFO,console &\n",
    "echo $! > hiveserver2.pid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-11 02:29:52,387 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "Archive:  trips.csv.zip\n",
      "  inflating: trips.csv               \n",
      "2021-01-11 02:30:01,807 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-11 02:30:02,637 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "[ ! -d \"/opt/datasets\" ] && mkdir /opt/datasets\n",
    "cd /opt/datasets\n",
    "\n",
    "wget -q -c https://tinyurl.com/y5roz8kz -O stations.csv\n",
    "hdfs dfs -mkdir -p bikeshare/stations\n",
    "hdfs dfs -put stations.csv bikeshare/stations\n",
    "\n",
    "wget -q -c https://tinyurl.com/y6ln8smc -O trips.csv.zip\n",
    "unzip trips.csv.zip\n",
    "rm trips.csv.zip\n",
    "hdfs dfs -mkdir -p bikeshare/trips\n",
    "hdfs dfs -put trips.csv bikeshare/trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect with beeline\n",
    "\n",
    "1. Run beeline\n",
    "\n",
    "```bash\n",
    "docker exec -it -u hadoop -w /opt hadoop bash -c \"source /opt/envvars.sh; beeline -n hadoop -u jdbc:hive2://localhost:10000\"\n",
    "```\n",
    "\n",
    "2. Configure jobs executor\n",
    "\n",
    "```sql\n",
    "SET hive.execution.engine=mr;\n",
    "SET mapreduce.framework.name=yarn;\n",
    "```\n",
    "\n",
    "3. Create bikeshare database\n",
    "\n",
    "```sql\n",
    "CREATE DATABASE bikeshare;\n",
    "SHOW DATABASES;\n",
    "USE bikeshare;\n",
    "```\n",
    "\n",
    "4. Create stations table\n",
    "\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE stations (\n",
    "station_id INT,\n",
    "name STRING,\n",
    "lat DOUBLE,\n",
    "long DOUBLE,\n",
    "dockcount INT,\n",
    "landmark STRING,\n",
    "installation STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs:///user/hadoop/bikeshare/stations';\n",
    "```\n",
    "\n",
    "5. Create trips table\n",
    "\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE trips (\n",
    "trip_id INT,\n",
    "duration INT,\n",
    "start_date STRING,\n",
    "start_station STRING,\n",
    "start_terminal INT,\n",
    "end_date STRING,\n",
    "end_station STRING,\n",
    "end_terminal INT,\n",
    "bike_num INT,\n",
    "subscription_type STRING,\n",
    "zip_code STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 'hdfs:///user/hadoop/bikeshare/trips';\n",
    "```\n",
    "\n",
    "6. Show tables\n",
    "\n",
    "```sql\n",
    "SHOW TABLES;\n",
    "DESCRIBE stations;\n",
    "DESCRIBE trips;\n",
    "DESCRIBE FORMATTED stations;\n",
    "DESCRIBE FORMATTED trips;\n",
    "```\n",
    "\n",
    "7. Run query - number of trips per terminal\n",
    "\n",
    "```sql\n",
    "SELECT start_terminal, start_station, COUNT(1) AS count\n",
    "FROM trips\n",
    "GROUP BY start_terminal, start_station\n",
    "ORDER BY count\n",
    "DESC LIMIT 10;\n",
    "```\n",
    "\n",
    "8. Run query - join between stations and trips\n",
    "\n",
    "```sql\n",
    "SELECT t.trip_id, t.duration, t.start_date, s.name, s.lat, s.long, s.landmark\n",
    "FROM stations s\n",
    "JOIN trips t ON s.station_id = t.start_terminal\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "9. Exit beeline\n",
    "\n",
    "```sql\n",
    "!quit\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "\n",
    "cd /opt/hive/hiveserver2\n",
    "\n",
    "# kill hiveserver2\n",
    "kill $(cat hiveserver2.pid)\n",
    "rm hiveserver2.pid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
