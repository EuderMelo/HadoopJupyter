{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - Web Interface\n",
    "\n",
    "- Master node\n",
    "    - NameNode: http://localhost:9870\n",
    "    - Secondary NameNode: http://localhost:9868\n",
    "- Worker node\n",
    "    - hadoop1\n",
    "        - NodeManager: http://localhost:8042\n",
    "    - hadoop2\n",
    "        - NodeManager: http://localhost:8043\n",
    "    - hadoop3\n",
    "        - NodeManager: http://localhost:8044"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS - CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystem Basic Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "Download books from Gutenberg project (http://www.gutenberg.org/)\n",
    "\n",
    "- Moby Dick; Or, The Whale by Herman Melville\n",
    "- Pride and Prejudice by Jane Austen\n",
    "- Dracula by Bram Stoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dracula.txt\n",
      "mobydick.txt\n",
      "prideandprejudice.txt\n",
      "stations.csv\n",
      "trips.csv.zip\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "\n",
    "[ ! -d \"/opt/datasets\" ] && mkdir /opt/datasets\n",
    "cd /opt/datasets\n",
    "\n",
    "wget -qc http://www.gutenberg.org/files/2701/2701-0.txt -O mobydick.txt\n",
    "wget -qc http://www.gutenberg.org/files/1342/1342-0.txt -O prideandprejudice.txt\n",
    "wget -qc http://www.gutenberg.org/cache/epub/345/pg345.txt -O dracula.txt\n",
    "    \n",
    "ls /opt/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-06 12:05:35,513 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-06 12:05:35,866 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-06 12:05:35,980 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "Found 3 items\n",
      "-rw-r--r--   2 hadoop hadoop     799738 2021-01-06 12:05 /user/hadoop/gutenberg/1342-0.txt\n",
      "-rw-r--r--   2 hadoop hadoop    1276201 2021-01-06 12:05 /user/hadoop/gutenberg/2701-0.txt\n",
      "-rw-r--r--   2 hadoop hadoop     883160 2021-01-06 12:05 /user/hadoop/gutenberg/pg345.txt\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "cd /opt/datasets\n",
    "\n",
    "# create gutenberg folder in HDFS\n",
    "# hdfs dfs -mkdir /user/hadoop/gutenberg\n",
    "\n",
    "# copy books to HDFS\n",
    "# hdfs dfs -put * /user/hadoop/gutenberg\n",
    "# hdfs dfs -copyFromLocal * /user/hadoop/gutenberg\n",
    "\n",
    "# list files in HDFS\n",
    "# hdfs dfs -ls /user/hadoop/gutenberg\n",
    "\n",
    "# show first KB of file\n",
    "# hdfs dfs -head /user/hadoop/gutenberg/mobydick.txt\n",
    "\n",
    "# show last KB of file\n",
    "# hdfs dfs -tail /user/hadoop/gutenberg/prideandprejudice.txt\n",
    "\n",
    "# show whole file - CAREFUL\n",
    "# hdfs dfs -cat /user/hadoop/gutenberg/dracula.txt\n",
    "\n",
    "# append file contents to a file in HDFS\n",
    "# hdfs dfs -appendToFile mobydick.txt prideandprejudice.txt dracula.txt /user/hadoop/allbooks.txt\n",
    "\n",
    "# copy allbooks.txt (in HDFS) to gutenberg directory (in HDFS)\n",
    "# hdfs dfs -cp allbooks.txt /user/hadoop/gutenberg\n",
    "# hdfs dfs -ls -h -R\n",
    "\n",
    "# retrieve allbooks.txt from HDFS\n",
    "# hdfs dfs -get allbooks.txt .\n",
    "# hdfs dfs -copyToLocal /user/hadoop/allbooks.txt .\n",
    "\n",
    "# remove file\n",
    "# hdfs dfs -rm allbooks.txt\n",
    "# hdfs dfs -rm /user/hadoop/allbooks.txt\n",
    "\n",
    "# mv file (also used for renaming)\n",
    "# hdfs dfs -mv gutenberg/allbooks.txt gutenberg/books.txt\n",
    "\n",
    "# print statistics on folder\n",
    "# printf \"name\\ttype\\tsize\\treps\\n\"\n",
    "# hdfs dfs -stat \"%n %F %b %r\" /user/hadoop/gutenberg/*\n",
    "\n",
    "# getmerge\n",
    "# hdfs dfs -getmerge /user/hadoop/gutenberg mergebooks.txt\n",
    "\n",
    "# remove directory and files (-R recursive)\n",
    "# hdfs dfs -rm -R /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilization in a MapReduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-08 12:54:59,516 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-08 12:54:59,870 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-08 12:54:59,976 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "cd /opt/datasets\n",
    "hdfs dfs -mkdir /user/hadoop/gutenberg\n",
    "hdfs dfs -put mobydick.txt prideandprejudice.txt dracula.txt /user/hadoop/gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-06 23:44:30,523 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-06 23:44:30,813 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-06 23:44:30,910 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-06 23:44:35,245 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-06 23:44:35,627 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-01-06 23:44:36,192 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1609940116821_0001\n",
      "2021-01-06 23:44:36,511 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-06 23:44:36,861 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2021-01-06 23:44:37,015 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-06 23:44:37,111 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-06 23:44:37,158 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2021-01-06 23:44:37,740 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-06 23:44:37,817 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1609940116821_0001\n",
      "2021-01-06 23:44:37,818 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-01-06 23:44:38,372 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-01-06 23:44:38,378 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-01-06 23:44:39,406 INFO impl.YarnClientImpl: Submitted application application_1609940116821_0001\n",
      "2021-01-06 23:44:39,554 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1609940116821_0001/\n",
      "2021-01-06 23:44:39,556 INFO mapreduce.Job: Running job: job_1609940116821_0001\n",
      "2021-01-06 23:44:59,004 INFO mapreduce.Job: Job job_1609940116821_0001 running in uber mode : false\n",
      "2021-01-06 23:44:59,008 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-01-06 23:45:17,731 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2021-01-06 23:45:18,774 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-01-06 23:45:30,014 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-01-06 23:45:31,050 INFO mapreduce.Job: Job job_1609940116821_0001 completed successfully\n",
      "2021-01-06 23:45:31,252 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=983104\n",
      "\t\tFILE: Number of bytes written=2870829\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2959444\n",
      "\t\tHDFS: Number of bytes written=566225\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=102662\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16814\n",
      "\t\tTotal time spent by all map tasks (ms)=51331\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8407\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=51331\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=8407\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=13140736\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2152192\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=52900\n",
      "\t\tMap output records=504959\n",
      "\t\tMap output bytes=4838678\n",
      "\t\tMap output materialized bytes=983116\n",
      "\t\tInput split bytes=345\n",
      "\t\tCombine input records=504959\n",
      "\t\tCombine output records=66277\n",
      "\t\tReduce input groups=50206\n",
      "\t\tReduce shuffle bytes=983116\n",
      "\t\tReduce input records=66277\n",
      "\t\tReduce output records=50206\n",
      "\t\tSpilled Records=132554\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=583\n",
      "\t\tCPU time spent (ms)=14230\n",
      "\t\tPhysical memory (bytes) snapshot=985161728\n",
      "\t\tVirtual memory (bytes) snapshot=7648702464\n",
      "\t\tTotal committed heap usage (bytes)=656932864\n",
      "\t\tPeak Map Physical memory (bytes)=284209152\n",
      "\t\tPeak Map Virtual memory (bytes)=1910030336\n",
      "\t\tPeak Reduce Physical memory (bytes)=156086272\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1918746624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2959099\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=566225\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# run wordcount application\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.1.jar wordcount \\\n",
    "/user/hadoop/gutenberg /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2021-01-06 23:45 /user/hadoop/gutenberg-output/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop     566225 2021-01-06 23:45 /user/hadoop/gutenberg-output/part-r-00000\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# list output folder contents\n",
    "hdfs dfs -ls /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-06 23:46:27,474 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "\"'Are\t1\n",
      "\"'E's\t1\n",
      "\"'I\t1\n",
      "\"'Ittin'\t1\n",
      "\"'Little\t1\n",
      "\"'Lucy,\t1\n",
      "\"'Maybe\t1\n",
      "\"'Miss\t1\n",
      "\"'My\t2\n",
      "\"'Never\t1\n",
      "\"'No'\t1\n",
      "\"'Ow\t1\n",
      "\"'Silence!\t1\n",
      "\"'That's\t1\n",
      "\"'Tyke\t1\n",
      "\"'Wilhelmina'--I\t1\n",
      "\"'Yes,\t1\n",
      "\"A\t8\n",
      "\"ABRAHAM\t1\n",
      "\"ART.\"\t1\n",
      "\"ARTHUR.\"\t1\n",
      "\"About\t1\n",
      "\"Afraid\t1\n",
      "\"Again\t1\n",
      "\"Agreed!\"\t1\n",
      "\"Ah\t2\n",
      "\"Ah,\t16\n",
      "\"Aha!\t1\n",
      "\"Aha!\"\t2\n",
      "\"Alas!\t1\n",
      "\"All\t7\n",
      "\"Already?\"\t1\n",
      "\"Am\t2\n",
      "\"Amen\"\t1\n",
      "\"An\t1\n",
      "\"An'\t1\n",
      "\"And\t49\n",
      "\"And,\t1\n",
      "\"Answer\t1\n",
      "\"Any\t1\n",
      "\"Arabian\t1\n",
      "\"Are\t5\n",
      "\"Arthur\t1\n",
      "\"Arthur!\t2\n",
      "\"As\t6\n",
      "\"Ask\t1\n",
      "\"At\t3\n",
      "\"Ay,\t1\n",
      "\"Back,\t1\n",
      "\"Be\t1\n",
      "\"Because\t7\n",
      "\"Because,\t1\n",
      "\"Because,\"\t2\n",
      "\"Before\t1\n",
      "\"Believe\t3\n",
      "\"Besides,\t1\n",
      "\"Bloofer\t1\n",
      "\"Blow\t1\n",
      "\"Blue\"\t1\n",
      "\"Bother\t1\n",
      "\"Brave\t2\n",
      "\"Bring\t1\n",
      "\"But\t21\n",
      "\"But,\t6\n",
      "\"But,\"\t4\n",
      "\"By\t3\n",
      "\"Call\t1\n",
      "\"Can\t3\n",
      "\"Can't\t1\n",
      "\"Certainly\t2\n",
      "\"Certainly,\"\t1\n",
      "\"Certainly.\"\t1\n",
      "\"Charcot\t1\n",
      "\"Come\t5\n",
      "\"Come!\"\t3\n",
      "\"Come,\t6\n",
      "\"Come,\"\t4\n",
      "\"Come.\t1\n",
      "\"Count\t4\n",
      "\"DEMETER.\"\t1\n",
      "\"DRACULA.\"\t1\n",
      "\"Dear\t5\n",
      "\"Defects,\"\t1\n",
      "\"Denn\t1\n",
      "\"Destroyed?\"\t1\n",
      "\"Did\t3\n",
      "\"Do\t19\n",
      "\"Doctor,\t1\n",
      "\"Don't\t3\n",
      "\"Dr.\t12\n",
      "\"Draw\t1\n",
      "\"Edward\t1\n",
      "\"Euthanasia\"\t1\n",
      "\"Ever\t1\n",
      "\"Exactly.\t1\n",
      "\"Excuse\t1\n",
      "\"FINIS.\"\t1\n",
      "\"Faithfully\t1\n",
      "\"Fear\t1\n",
      "\"Fifty\t2\n",
      "\"Finis,\"\t1\n",
      "\"For\t5\n",
      "\"Forgive\t4\n",
      "\"Forgiven!\t1\n",
      "\"Frankly\t1\n",
      "\"Friend\t10\n",
      "\"Give\t1\n",
      "\"G"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# show head\n",
    "hdfs dfs -head /user/hadoop/gutenberg-output/part-r-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-06 23:46:49,821 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "cd /tmp\n",
    "\n",
    "# copy HDFS file to local filesystem\n",
    "hdfs dfs -get /user/hadoop/gutenberg-output/part-r-00000 gutenberg-output.txt\n",
    "head /tmp/gutenberg-output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/hadoop/gutenberg-output\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# remove folder on HDFS\n",
    "hdfs dfs -rm -R /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-08 12:55:19,796 INFO client.RMProxy: Connecting to ResourceManager at hadoop/172.17.0.2:8032\n",
      "2021-01-08 12:55:20,214 INFO client.AHSProxy: Connecting to Application History server at hadoop/172.17.0.2:10200\n",
      "2021-01-08 12:55:20,672 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1610022874613_0027\n",
      "2021-01-08 12:55:20,992 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-08 12:55:21,746 INFO input.FileInputFormat: Total input files to process : 3\n",
      "2021-01-08 12:55:21,830 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-08 12:55:21,914 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-08 12:55:21,957 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "2021-01-08 12:55:22,439 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-08 12:55:22,559 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1610022874613_0027\n",
      "2021-01-08 12:55:22,559 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-01-08 12:55:23,398 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-01-08 12:55:23,400 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-01-08 12:55:24,013 INFO impl.YarnClientImpl: Submitted application application_1610022874613_0027\n",
      "2021-01-08 12:55:24,141 INFO mapreduce.Job: The url to track the job: http://hadoop:8088/proxy/application_1610022874613_0027/\n",
      "2021-01-08 12:55:24,147 INFO mapreduce.Job: Running job: job_1610022874613_0027\n",
      "2021-01-08 12:55:40,900 INFO mapreduce.Job: Job job_1610022874613_0027 running in uber mode : false\n",
      "2021-01-08 12:55:40,903 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-01-08 12:55:58,561 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2021-01-08 12:56:05,014 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2021-01-08 12:56:07,234 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-01-08 12:56:19,387 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2021-01-08 12:56:23,793 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-01-08 12:56:23,823 INFO mapreduce.Job: Job job_1610022874613_0027 completed successfully\n",
      "2021-01-08 12:56:24,044 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=983110\n",
      "\t\tFILE: Number of bytes written=3096171\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2959461\n",
      "\t\tHDFS: Number of bytes written=566225\n",
      "\t\tHDFS: Number of read operations=19\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=122822\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=81146\n",
      "\t\tTotal time spent by all map tasks (ms)=61411\n",
      "\t\tTotal time spent by all reduce tasks (ms)=40573\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=61411\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=40573\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15721216\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=10386688\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=52900\n",
      "\t\tMap output records=504959\n",
      "\t\tMap output bytes=4838678\n",
      "\t\tMap output materialized bytes=983134\n",
      "\t\tInput split bytes=362\n",
      "\t\tCombine input records=504959\n",
      "\t\tCombine output records=66277\n",
      "\t\tReduce input groups=50206\n",
      "\t\tReduce shuffle bytes=983134\n",
      "\t\tReduce input records=66277\n",
      "\t\tReduce output records=50206\n",
      "\t\tSpilled Records=132554\n",
      "\t\tShuffled Maps =6\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=6\n",
      "\t\tGC time elapsed (ms)=987\n",
      "\t\tCPU time spent (ms)=23120\n",
      "\t\tPhysical memory (bytes) snapshot=1193521152\n",
      "\t\tVirtual memory (bytes) snapshot=9578921984\n",
      "\t\tTotal committed heap usage (bytes)=759169024\n",
      "\t\tPeak Map Physical memory (bytes)=287379456\n",
      "\t\tPeak Map Virtual memory (bytes)=1913462784\n",
      "\t\tPeak Reduce Physical memory (bytes)=169631744\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1923559424\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2959099\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=566225\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "cd /opt/hadoop/share/hadoop/mapreduce\n",
    "\n",
    "# run wordcount application with 2 reducers\n",
    "hadoop jar ./hadoop-mapreduce-examples-3.2.1.jar wordcount \\\n",
    "-Dmapreduce.job.reduces=2 \\\n",
    "/user/hadoop/gutenberg /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2021-01-08 12:56 /user/hadoop/gutenberg-output/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop     282621 2021-01-08 12:56 /user/hadoop/gutenberg-output/part-r-00000\n",
      "-rw-r--r--   2 hadoop hadoop     283604 2021-01-08 12:56 /user/hadoop/gutenberg-output/part-r-00001\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# list output folder contents\n",
    "hdfs dfs -ls /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-08 12:57:29,087 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-01-08 12:57:29,309 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "\"'Are\t1\n",
      "\"'Little\t1\n",
      "\"'Maybe\t1\n",
      "\"'Miss\t1\n",
      "\"'My\t2\n",
      "\"'Never\t1\n",
      "\"'No'\t1\n",
      "\"'Ow\t1\n",
      "\"'Silence!\t1\n",
      "\"'Wilhelmina'--I\t1\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "cd /tmp\n",
    "\n",
    "# copy HDFS file to local filesystem\n",
    "hdfs dfs -getmerge /user/hadoop/gutenberg-output gutenberg-output.txt\n",
    "head /tmp/gutenberg-output.txt\n",
    "\n",
    "hdfs dfs -rm -R /user/hadoop/gutenberg-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Commands\n",
    "\n",
    "- https://hadoop.apache.org/docs/r3.2.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify HDFS cluster status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rack: /default-rack\n",
      "   172.17.0.3:9866 (hadoop1)\n",
      "   172.17.0.4:9866 (hadoop2)\n",
      "   172.17.0.5:9866 (hadoop3)\n",
      "\n",
      "========================================\n",
      "Configured Capacity: 188176871424 (175.25 GB)\n",
      "Present Capacity: 60270764530 (56.13 GB)\n",
      "DFS Remaining: 60262526976 (56.12 GB)\n",
      "DFS Used: 8237554 (7.86 MB)\n",
      "DFS Used%: 0.01%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 1757678 (1.68 MB)\n",
      "Non DFS Used: 39419641362 (36.71 GB)\n",
      "DFS Remaining: 20087508992 (18.71 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 23:47:49 GMT 2021\n",
      "Last Block Report: Wed Jan 06 20:13:58 GMT 2021\n",
      "Num of Blocks: 8\n",
      "\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 2995634 (2.86 MB)\n",
      "Non DFS Used: 39418403406 (36.71 GB)\n",
      "DFS Remaining: 20087508992 (18.71 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 23:47:49 GMT 2021\n",
      "Last Block Report: Wed Jan 06 23:35:38 GMT 2021\n",
      "Num of Blocks: 9\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 3484242 (3.32 MB)\n",
      "Non DFS Used: 39417914798 (36.71 GB)\n",
      "DFS Remaining: 20087508992 (18.71 GB)\n",
      "DFS Used%: 0.01%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 23:47:49 GMT 2021\n",
      "Last Block Report: Wed Jan 06 19:56:40 GMT 2021\n",
      "Num of Blocks: 7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# print topology\n",
    "hdfs dfsadmin -printTopology\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" |tr \" \" \"=\"\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replication factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://hadoop:9870/fsck?ugi=hadoop&files=1&blocks=1&locations=1&path=%2Fuser%2Fhadoop%2Fgutenberg\n",
      "FSCK started by hadoop (auth:SIMPLE) from /172.17.0.2 for path /user/hadoop/gutenberg at Wed Jan 06 23:50:47 GMT 2021\n",
      "/user/hadoop/gutenberg <dir>\n",
      "/user/hadoop/gutenberg/book1.txt 1276201 bytes, replicated: replication=3, 1 block(s):  OK\n",
      "0. BP-361701204-172.17.0.2-1609934326274:blk_1073741862_1038 len=1276201 Live_repl=3  [DatanodeInfoWithStorage[172.17.0.5:9866,DS-5840172a-b11f-4b67-ba2a-dc19c050287b,DISK], DatanodeInfoWithStorage[172.17.0.4:9866,DS-48d422f2-7e3f-402f-9948-1a4d8a13840a,DISK], DatanodeInfoWithStorage[172.17.0.3:9866,DS-d88ef73a-59ee-4c59-bcf5-d23bc4c680af,DISK]]\n",
      "\n",
      "/user/hadoop/gutenberg/book2.txt 799738 bytes, replicated: replication=3, 1 block(s):  OK\n",
      "0. BP-361701204-172.17.0.2-1609934326274:blk_1073741863_1039 len=799738 Live_repl=3  [DatanodeInfoWithStorage[172.17.0.5:9866,DS-5840172a-b11f-4b67-ba2a-dc19c050287b,DISK], DatanodeInfoWithStorage[172.17.0.4:9866,DS-48d422f2-7e3f-402f-9948-1a4d8a13840a,DISK], DatanodeInfoWithStorage[172.17.0.3:9866,DS-d88ef73a-59ee-4c59-bcf5-d23bc4c680af,DISK]]\n",
      "\n",
      "/user/hadoop/gutenberg/book3.txt 883160 bytes, replicated: replication=3, 1 block(s):  OK\n",
      "0. BP-361701204-172.17.0.2-1609934326274:blk_1073741864_1040 len=883160 Live_repl=3  [DatanodeInfoWithStorage[172.17.0.5:9866,DS-5840172a-b11f-4b67-ba2a-dc19c050287b,DISK], DatanodeInfoWithStorage[172.17.0.3:9866,DS-d88ef73a-59ee-4c59-bcf5-d23bc4c680af,DISK], DatanodeInfoWithStorage[172.17.0.4:9866,DS-48d422f2-7e3f-402f-9948-1a4d8a13840a,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t3\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t1\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t2959099 B\n",
      " Total files:\t3\n",
      " Total blocks (validated):\t3 (avg. block size 986366 B)\n",
      " Minimally replicated blocks:\t3 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t3.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      "FSCK ended at Wed Jan 06 23:50:47 GMT 2021 in 5 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/user/hadoop/gutenberg' is HEALTHY\n",
      "\n",
      "========================================\n",
      "\n",
      "Replication 3 set: /user/hadoop/gutenberg/book1.txt\n",
      "Replication 3 set: /user/hadoop/gutenberg/book2.txt\n",
      "Replication 3 set: /user/hadoop/gutenberg/book3.txt\n",
      "\n",
      "========================================\n",
      "\n",
      "Connecting to namenode via http://hadoop:9870/fsck?ugi=hadoop&files=1&blocks=1&locations=1&path=%2Fuser%2Fhadoop%2Fgutenberg\n",
      "FSCK started by hadoop (auth:SIMPLE) from /172.17.0.2 for path /user/hadoop/gutenberg at Wed Jan 06 23:50:57 GMT 2021\n",
      "/user/hadoop/gutenberg <dir>\n",
      "/user/hadoop/gutenberg/book1.txt 1276201 bytes, replicated: replication=3, 1 block(s):  OK\n",
      "0. BP-361701204-172.17.0.2-1609934326274:blk_1073741862_1038 len=1276201 Live_repl=3  [DatanodeInfoWithStorage[172.17.0.5:9866,DS-5840172a-b11f-4b67-ba2a-dc19c050287b,DISK], DatanodeInfoWithStorage[172.17.0.4:9866,DS-48d422f2-7e3f-402f-9948-1a4d8a13840a,DISK], DatanodeInfoWithStorage[172.17.0.3:9866,DS-d88ef73a-59ee-4c59-bcf5-d23bc4c680af,DISK]]\n",
      "\n",
      "/user/hadoop/gutenberg/book2.txt 799738 bytes, replicated: replication=3, 1 block(s):  OK\n",
      "0. BP-361701204-172.17.0.2-1609934326274:blk_1073741863_1039 len=799738 Live_repl=3  [DatanodeInfoWithStorage[172.17.0.5:9866,DS-5840172a-b11f-4b67-ba2a-dc19c050287b,DISK], DatanodeInfoWithStorage[172.17.0.4:9866,DS-48d422f2-7e3f-402f-9948-1a4d8a13840a,DISK], DatanodeInfoWithStorage[172.17.0.3:9866,DS-d88ef73a-59ee-4c59-bcf5-d23bc4c680af,DISK]]\n",
      "\n",
      "/user/hadoop/gutenberg/book3.txt 883160 bytes, replicated: replication=3, 1 block(s):  OK\n",
      "0. BP-361701204-172.17.0.2-1609934326274:blk_1073741864_1040 len=883160 Live_repl=3  [DatanodeInfoWithStorage[172.17.0.5:9866,DS-5840172a-b11f-4b67-ba2a-dc19c050287b,DISK], DatanodeInfoWithStorage[172.17.0.3:9866,DS-d88ef73a-59ee-4c59-bcf5-d23bc4c680af,DISK], DatanodeInfoWithStorage[172.17.0.4:9866,DS-48d422f2-7e3f-402f-9948-1a4d8a13840a,DISK]]\n",
      "\n",
      "\n",
      "Status: HEALTHY\n",
      " Number of data-nodes:\t3\n",
      " Number of racks:\t\t1\n",
      " Total dirs:\t\t\t1\n",
      " Total symlinks:\t\t0\n",
      "\n",
      "Replicated Blocks:\n",
      " Total size:\t2959099 B\n",
      " Total files:\t3\n",
      " Total blocks (validated):\t3 (avg. block size 986366 B)\n",
      " Minimally replicated blocks:\t3 (100.0 %)\n",
      " Over-replicated blocks:\t0 (0.0 %)\n",
      " Under-replicated blocks:\t0 (0.0 %)\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\n",
      " Default replication factor:\t2\n",
      " Average block replication:\t3.0\n",
      " Missing blocks:\t\t0\n",
      " Corrupt blocks:\t\t0\n",
      " Missing replicas:\t\t0 (0.0 %)\n",
      "\n",
      "Erasure Coded Block Groups:\n",
      " Total size:\t0 B\n",
      " Total files:\t0\n",
      " Total block groups (validated):\t0\n",
      " Minimally erasure-coded block groups:\t0\n",
      " Over-erasure-coded block groups:\t0\n",
      " Under-erasure-coded block groups:\t0\n",
      " Unsatisfactory placement block groups:\t0\n",
      " Average block group size:\t0.0\n",
      " Missing block groups:\t\t0\n",
      " Corrupt block groups:\t\t0\n",
      " Missing internal blocks:\t0\n",
      "FSCK ended at Wed Jan 06 23:50:57 GMT 2021 in 9 milliseconds\n",
      "\n",
      "\n",
      "The filesystem under path '/user/hadoop/gutenberg' is HEALTHY\n",
      "\n",
      "========================================\n",
      "\n",
      "Replication 2 set: /user/hadoop/gutenberg/book1.txt\n",
      "Replication 2 set: /user/hadoop/gutenberg/book2.txt\n",
      "Replication 2 set: /user/hadoop/gutenberg/book3.txt\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# list folder block location\n",
    "hdfs fsck /user/hadoop/gutenberg -files -blocks -locations\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" |tr \" \" \"=\"\n",
    "\n",
    "# change replication factor of all files in directory to 3\n",
    "hdfs dfs -setrep 3 /user/hadoop/gutenberg\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" |tr \" \" \"=\"\n",
    "\n",
    "# list folder block location\n",
    "hdfs fsck /user/hadoop/gutenberg -files -blocks -locations\n",
    "\n",
    "printf \"\\n%40s\\n\\n\" |tr \" \" \"=\"\n",
    "\n",
    "# change replication factor back to 2\n",
    "hdfs dfs -setrep 2 /user/hadoop/gutenberg\n",
    "\n",
    "# list folder block location\n",
    "hdfs fsck /user/hadoop/gutenberg -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomission nodes\n",
    "\n",
    "- dfs.hosts.exclude in hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh nodes successful\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Decomissioning hadoop1\n",
    "cat > /opt/hadoop/etc/hadoop/dfs.exclude << EOF\n",
    "hadoop1\n",
    "EOF\n",
    "\n",
    "hdfs dfsadmin -refreshNodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 125451247616 (116.84 GB)\n",
      "Present Capacity: 40175570893 (37.42 GB)\n",
      "DFS Remaining: 40167333888 (37.41 GB)\n",
      "DFS Used: 8237005 (7.86 MB)\n",
      "DFS Used%: 0.02%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Decommissioned\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 3849357 (3.67 MB)\n",
      "Non DFS Used: 39421391731 (36.71 GB)\n",
      "DFS Remaining: 20083666944 (18.70 GB)\n",
      "DFS Used%: 0.01%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 23:55:01 GMT 2021\n",
      "Last Block Report: Wed Jan 06 20:13:58 GMT 2021\n",
      "Num of Blocks: 10\n",
      "\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 4112333 (3.92 MB)\n",
      "Non DFS Used: 39421128755 (36.71 GB)\n",
      "DFS Remaining: 20083666944 (18.70 GB)\n",
      "DFS Used%: 0.01%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 23:55:01 GMT 2021\n",
      "Last Block Report: Wed Jan 06 23:35:38 GMT 2021\n",
      "Num of Blocks: 12\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 4124672 (3.93 MB)\n",
      "Non DFS Used: 39421116416 (36.71 GB)\n",
      "DFS Remaining: 20083666944 (18.70 GB)\n",
      "DFS Used%: 0.01%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 23:55:01 GMT 2021\n",
      "Last Block Report: Wed Jan 06 19:56:40 GMT 2021\n",
      "Num of Blocks: 12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# report HDFS status\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh nodes successful\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Recomission all nodes\n",
    "cat > /opt/hadoop/etc/hadoop/dfs.exclude << EOF\n",
    "EOF\n",
    "\n",
    "hdfs dfsadmin -refreshNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 125451247616 (116.84 GB)\n",
      "Present Capacity: 40182281386 (37.42 GB)\n",
      "DFS Remaining: 40174968832 (37.42 GB)\n",
      "DFS Used: 7312554 (6.97 MB)\n",
      "DFS Used%: 0.02%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (2):\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 3222266 (3.07 MB)\n",
      "Non DFS Used: 39418201350 (36.71 GB)\n",
      "DFS Remaining: 20087484416 (18.71 GB)\n",
      "DFS Used%: 0.01%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 23:57:55 GMT 2021\n",
      "Last Block Report: Wed Jan 06 23:35:38 GMT 2021\n",
      "Num of Blocks: 12\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 4090288 (3.90 MB)\n",
      "Non DFS Used: 39417333328 (36.71 GB)\n",
      "DFS Remaining: 20087484416 (18.71 GB)\n",
      "DFS Used%: 0.01%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 23:57:55 GMT 2021\n",
      "Last Block Report: Wed Jan 06 19:56:40 GMT 2021\n",
      "Num of Blocks: 12\n",
      "\n",
      "\n",
      "Dead datanodes (1):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 1007244 (983.64 KB)\n",
      "Non DFS Used: 39420383604 (36.71 GB)\n",
      "DFS Remaining: 20087517184 (18.71 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Wed Jan 06 23:56:52 GMT 2021\n",
      "Last Block Report: Wed Jan 06 20:13:58 GMT 2021\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# report HDFS status\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling datanode failures\n",
    "\n",
    "- timeouts defined in hdfs-site.xml \n",
    "    - dfs.namenode.heartbeat.recheck-interval = 10000 (10 seconds)\n",
    "    - dfs.heartbeat.interval = 3 seconds\n",
    "- timeout = 2 x recheck-interval + 10 x heartbeat.interval\n",
    "    - timeout = 50 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "3s\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# get dfs.namenode.heartbeat.recheck-interval\n",
    "hdfs getconf -confKey dfs.namenode.heartbeat.recheck-interval\n",
    "\n",
    "# get dfs.heartbeat.interval\n",
    "hdfs getconf -confKey dfs.heartbeat.interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# simulate node fault\n",
    "docker pause hadoop1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:9870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 125451247616 (116.84 GB)\n",
      "Present Capacity: 40179048448 (37.42 GB)\n",
      "DFS Remaining: 40171765760 (37.41 GB)\n",
      "DFS Used: 7282688 (6.95 MB)\n",
      "DFS Used%: 0.02%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (2):\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 3641344 (3.47 MB)\n",
      "Non DFS Used: 39419383808 (36.71 GB)\n",
      "DFS Remaining: 20085882880 (18.71 GB)\n",
      "DFS Used%: 0.01%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 13:52:10 GMT 2021\n",
      "Last Block Report: Wed Jan 06 13:34:41 GMT 2021\n",
      "Num of Blocks: 8\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 3641344 (3.47 MB)\n",
      "Non DFS Used: 39419383808 (36.71 GB)\n",
      "DFS Remaining: 20085882880 (18.71 GB)\n",
      "DFS Used%: 0.01%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 13:52:13 GMT 2021\n",
      "Last Block Report: Wed Jan 06 13:34:41 GMT 2021\n",
      "Num of Blocks: 8\n",
      "\n",
      "\n",
      "Dead datanodes (1):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 3641344 (3.47 MB)\n",
      "Non DFS Used: 39415738368 (36.71 GB)\n",
      "DFS Remaining: 20089528320 (18.71 GB)\n",
      "DFS Used%: 0.01%\n",
      "DFS Remaining%: 32.03%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Wed Jan 06 13:50:53 GMT 2021\n",
      "Last Block Report: Wed Jan 06 13:47:33 GMT 2021\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# recover node\n",
    "docker unpause hadoop1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 188176871424 (175.25 GB)\n",
      "Present Capacity: 60270694349 (56.13 GB)\n",
      "DFS Remaining: 60262416384 (56.12 GB)\n",
      "DFS Used: 8277965 (7.89 MB)\n",
      "DFS Used%: 0.01%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (3):\n",
      "\n",
      "Name: 172.17.0.3:9866 (hadoop1)\n",
      "Hostname: hadoop1\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 40960 (40 KB)\n",
      "Non DFS Used: 39421394944 (36.71 GB)\n",
      "DFS Remaining: 20087472128 (18.71 GB)\n",
      "DFS Used%: 0.00%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 23:59:13 GMT 2021\n",
      "Last Block Report: Wed Jan 06 23:58:52 GMT 2021\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n",
      "Name: 172.17.0.4:9866 (hadoop2)\n",
      "Hostname: hadoop2\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 4112333 (3.92 MB)\n",
      "Non DFS Used: 39417323571 (36.71 GB)\n",
      "DFS Remaining: 20087472128 (18.71 GB)\n",
      "DFS Used%: 0.01%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 23:59:13 GMT 2021\n",
      "Last Block Report: Wed Jan 06 23:35:38 GMT 2021\n",
      "Num of Blocks: 12\n",
      "\n",
      "\n",
      "Name: 172.17.0.5:9866 (hadoop3)\n",
      "Hostname: hadoop3\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 62725623808 (58.42 GB)\n",
      "DFS Used: 4124672 (3.93 MB)\n",
      "Non DFS Used: 39417311232 (36.71 GB)\n",
      "DFS Remaining: 20087472128 (18.71 GB)\n",
      "DFS Used%: 0.01%\n",
      "DFS Remaining%: 32.02%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Wed Jan 06 23:59:13 GMT 2021\n",
      "Last Block Report: Wed Jan 06 19:56:40 GMT 2021\n",
      "Num of Blocks: 12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfsadmin -report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
