{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dockermagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "![Spark](https://spark.apache.org/images/spark-logo-trademark.png)\n",
    "\n",
    "- https://spark.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- version 3.0.1 (Pre-built for Apache Hadoop 3.2 and later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download package\n",
    "wget -q -c https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
    "\n",
    "# Copy installation package to container\n",
    "docker cp spark-3.0.1-bin-hadoop3.2.tgz hadoop:/opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Spark\n",
      "export SPARK_HOME=/opt/spark\n",
      "export PYSPARK_PYTHON=python3\n",
      "export PYSPARK_DRIVER_PYTHON=ipython3\n",
      "export PYTHONIOENCODING=utf8\n",
      "#export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
      "export PATH=$PATH:$SPARK_HOME/bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "\n",
    "# unpack file and create link\n",
    "tar -zxf /opt/spark-3.0.1-bin-hadoop3.2.tgz -C /opt\n",
    "ln -s /opt/spark-3.0.1-bin-hadoop3.2 /opt/spark\n",
    "\n",
    "# update envvars.sh\n",
    "tee -a /opt/envvars.sh << EOF\n",
    "# Spark\n",
    "export SPARK_HOME=/opt/spark\n",
    "export PYSPARK_PYTHON=python3\n",
    "export PYSPARK_DRIVER_PYTHON=ipython3\n",
    "export PYTHONIOENCODING=utf8\n",
    "#export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
    "export PATH=\\$PATH:\\$SPARK_HOME/bin\n",
    "\n",
    "EOF\n",
    "\n",
    "sudo rm /opt/spark-3.0.1-bin-hadoop3.2.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Missing example class name.\n",
      "\n",
      "Usage: ./bin/run-example [options] example-class [example args]\n",
      "\n",
      "Options:\n",
      "  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
      "                              k8s://https://host:port, or local (Default: local[*]).\n",
      "  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
      "                              on one of the worker machines inside the cluster (\"cluster\")\n",
      "                              (Default: client).\n",
      "  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
      "  --name NAME                 A name of your application.\n",
      "  --jars JARS                 Comma-separated list of jars to include on the driver\n",
      "                              and executor classpaths.\n",
      "  --packages                  Comma-separated list of maven coordinates of jars to include\n",
      "                              on the driver and executor classpaths. Will search the local\n",
      "                              maven repo, then maven central and any additional remote\n",
      "                              repositories given by --repositories. The format for the\n",
      "                              coordinates should be groupId:artifactId:version.\n",
      "  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
      "                              resolving the dependencies provided in --packages to avoid\n",
      "                              dependency conflicts.\n",
      "  --repositories              Comma-separated list of additional remote repositories to\n",
      "                              search for the maven coordinates given with --packages.\n",
      "  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
      "                              on the PYTHONPATH for Python apps.\n",
      "  --files FILES               Comma-separated list of files to be placed in the working\n",
      "                              directory of each executor. File paths of these files\n",
      "                              in executors can be accessed via SparkFiles.get(fileName).\n",
      "\n",
      "  --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n",
      "  --properties-file FILE      Path to a file from which to load extra properties. If not\n",
      "                              specified, this will look for conf/spark-defaults.conf.\n",
      "\n",
      "  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
      "  --driver-java-options       Extra Java options to pass to the driver.\n",
      "  --driver-library-path       Extra library path entries to pass to the driver.\n",
      "  --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
      "                              jars added with --jars are automatically included in the\n",
      "                              classpath.\n",
      "\n",
      "  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
      "\n",
      "  --proxy-user NAME           User to impersonate when submitting the application.\n",
      "                              This argument does not work with --principal / --keytab.\n",
      "\n",
      "  --help, -h                  Show this help message and exit.\n",
      "  --verbose, -v               Print additional debug output.\n",
      "  --version,                  Print the version of current Spark.\n",
      "\n",
      " Cluster deploy mode only:\n",
      "  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
      "                              (Default: 1).\n",
      "\n",
      " Spark standalone or Mesos with cluster deploy mode only:\n",
      "  --supervise                 If given, restarts the driver on failure.\n",
      "\n",
      " Spark standalone, Mesos or K8s with cluster deploy mode only:\n",
      "  --kill SUBMISSION_ID        If given, kills the driver specified.\n",
      "  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
      "\n",
      " Spark standalone, Mesos and Kubernetes only:\n",
      "  --total-executor-cores NUM  Total cores for all executors.\n",
      "\n",
      " Spark standalone, YARN and Kubernetes only:\n",
      "  --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n",
      "                              YARN and K8S modes, or all available cores on the worker\n",
      "                              in standalone mode).\n",
      "\n",
      " Spark on YARN and Kubernetes only:\n",
      "  --num-executors NUM         Number of executors to launch (Default: 2).\n",
      "                              If dynamic allocation is enabled, the initial number of\n",
      "                              executors will be at least NUM.\n",
      "  --principal PRINCIPAL       Principal to be used to login to KDC.\n",
      "  --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
      "                              principal specified above.\n",
      "\n",
      " Spark on YARN only:\n",
      "  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
      "  --archives ARCHIVES         Comma separated list of archives to be extracted into the\n",
      "                              working directory of each executor.\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "# Local execution\n",
    "# $SPARK_HOME/bin/run-example --master local SparkPi 10\n",
    "\n",
    "# Local execution with 4 processes\n",
    "# $SPARK_HOME/bin/run-example --master local[4] SparkPi 10\n",
    "\n",
    "# Execution using YARN\n",
    "# $SPARK_HOME/bin/run-example --master yarn SparkPi 10\n",
    "\n",
    "# Execution using spark-submit\n",
    "$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi --master local \\\n",
    "$SPARK_HOME/examples/jars/spark-examples_2.12-3.0.1.jar 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyspark\n",
    "\n",
    "```bash\n",
    "docker exec -it -u hadoop -w /opt hadoop bash -c \"source /opt/envvars.sh; pyspark --master yarn\"\n",
    "```\n",
    "\n",
    "- Spark application UI - http://localhost:4040\n",
    "\n",
    "```python\n",
    "text_file = sc.textFile(\"hdfs:///user/hadoop/shakespeare\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs:///user/hadoop/shakespeare_result\")\n",
    "counts.collect()\n",
    "```\n",
    "\n",
    "```python\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted shakespeare_result\n"
     ]
    }
   ],
   "source": [
    "%%dockerexec -u hadoop hadoop\n",
    "source /opt/envvars.sh\n",
    "\n",
    "hdfs dfs -rm -r shakespeare_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
